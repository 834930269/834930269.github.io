{"name":"爬虫","slug":"Python/未分类/爬虫","count":2,"postlist":[{"title":"股票数据定向爬虫","slug":"gupiaodingxiangpachong","date":"2017-09-07T12:05:07.000Z","updated":"2019-07-03T13:51:36.852Z","comments":true,"path":"api/articles/gupiaodingxiangpachong.json","excerpt":"","keywords":null,"cover":null,"content":"<p>累了…直接撩代码</p>\n<pre><code>import requests\nfrom bs4 import BeautifulSoup\nimport traceback\nimport re\n\ndef getHTMLText(url):\n    try:\n        r=requests.get(url)\n        r.raise_for_status()\n        r.encoding=r.apparent_encoding\n        return r.text\n    except:\n        return &quot;&quot;\n\ndef getStockList(lst,stockURL):\n    html=getHTMLText(stockURL)\n    soup=BeautifulSoup(html,&apos;html.parser&apos;)\n    a=soup.find_all(&apos;a&apos;)\n    for i in a:\n        try:\n            href=i.attrs[&apos;href&apos;]\n            lst.append(re.findall(r&apos;[s][hz]\\d{6}&apos;,href)[0])\n        except:\n            continue\n\ndef getStockInfo(lst,stockURL,fpath):\n    for stock in lst:\n        url=stockURL+stock+&quot;.html&quot;\n        html=getHTMLText(url)\n        try:\n            if html==&quot;&quot;:\n                continue\n            infoDict={}\n            soup=BeautifulSoup(html,&apos;html.parser&apos;)\n            stockInfo=soup.find(&apos;div&apos;,attrs={&apos;class&apos;:&apos;stock-bets&apos;})\n\n            name=stockInfo.find_all(attrs={&apos;class&apos;:&apos;bets-name&apos;})[0]\n            infoDict.update({&apos;股票名称&apos;:name.text.split()[0]})\n            keyList=stockInfo.find_all(&apos;dt&apos;)\n            valueList=stockInfo.find_all(&apos;dd&apos;)\n            for i in range(len(keyList)):\n                key=keyList[i].text\n                val=valueList[i].text\n                infoDict[key]=val\n\n            with open(fpath,&apos;a&apos;,encoding=&apos;utf-8&apos;) as f:\n                f.write(str(infoDict)+&apos;\\n&apos;)\n        except:\n            traceback.print_exc()\n            continue\n\nif __name__==&apos;__main__&apos;:\n    stock_list_url = &apos;http://quote.eastmoney.com/stocklist.html&apos;\n    stock_info_url = &apos;https://gupiao.baidu.com/stock/&apos;\n    output_file = &apos;E:\\学习相关\\廖雪峰\\python_study\\库\\第三章\\BaiduStockInfo.txt&apos;\n    slist=[]\n    getStockList(slist,stock_list_url)\n    getStockInfo(slist,stock_info_url,output_file)</code></pre><p>结果(超慢的得得得多多多多多多多,还没爬完…不过应该是解析全文默认编码的问题): 2017-09-07 20:08:30 星期四 :earth_asia:<a href=\"https://github.com/834930269/python_study/blob/master/%E5%BA%93/%E7%AC%AC%E4%B8%89%E7%AB%A0/BaiduStockInfo.txt\" title=\"github: 爬取结果.txt\" target=\"_blank\" rel=\"noopener\">github: 爬取结果.txt</a></p>\n","text":"累了…直接撩代码import requestsfrom bs4 import BeautifulSoupimport tracebackimport redef getHTMLText(url):    try:        r=requests.get(url)       ","link":"","raw":null,"photos":[],"categories":[{"name":"Python","slug":"Python","count":41,"path":"api/categories/Python.json"},{"name":"未分类","slug":"Python/未分类","count":3,"path":"api/categories/Python/未分类.json"},{"name":"爬虫","slug":"Python/未分类/爬虫","count":2,"path":"api/categories/Python/未分类/爬虫.json"}],"tags":[{"name":"Python","slug":"Python","count":65,"path":"api/tags/Python.json"},{"name":"爬虫","slug":"爬虫","count":4,"path":"api/tags/爬虫.json"}]},{"title":"淘宝竞价爬虫","slug":"taobao-bug","date":"2017-09-06T23:40:58.000Z","updated":"2019-07-03T13:51:36.852Z","comments":true,"path":"api/articles/taobao-bug.json","excerpt":"","keywords":null,"cover":"http://be-sunshine.cn/wp-content/uploads/2017/09/1-1.png","content":"<p>首先我们先观察搜索为书包的链接: 第一页: <code>https://s.taobao.com/search?q=书包&amp;imgfile=&amp;commend=all&amp;ssid=s5-e&amp;search_type=item&amp;sourceId=tb.index&amp;spm=a21bo.50862.201856-taobao-item.1&amp;ie=utf8&amp;initiative_id=tbindexz_20170906</code> 第二页: <code>https://s.taobao.com/search?q=书包imgfile=&amp;commend=all&amp;ssid=s5-e&amp;search_type=item&amp;sourceId=tb.index&amp;spm=a21bo.50862.201856-taobao-item.1&amp;ie=utf8&amp;initiative_id=tbindexz_20170906&amp;bcoffset=4&amp;ntoffset=4&amp;p4ppushleft=1%2C48&amp;s=44</code> 我们可以观察到书包的属性是q,而每页有44个商品,所以页数的属性是s. 所以我们推测当链接为: <code>http://s.taobao.com/search?q=书包&amp;s=44</code> 时,时当前搜索目录的第二页. 首先是getHTMLtext:</p>\n<pre><code>def getHTMLtext(url):\n    try:\n        r=requests.get(url)\n        r.raise_for_status()\n        r.encoding=r.apparent_encoding\n        return r.text\n    except:\n        print(&quot;&quot;)</code></pre><p>然后我们分析代码,比如第一页第一件商品的价钱如下: <img src=\"http://be-sunshine.cn/wp-content/uploads/2017/09/1-1.png\" alt=\"第一件\" title=\"第一件\"> 我们在代码页搜索对应的价格89.00,发现对应的标记是’view_price’: <img src=\"http://be-sunshine.cn/wp-content/uploads/2017/09/2-1.png\" alt=\"89.00\" title=\"89.00\"> 然后是对应的标题,这里有一些问题,因为第一个东西的标题和显示的对不上,显示的是放在’title’标记中,而不知道什么的标题放在’raw_title’中: <img src=\"http://be-sunshine.cn/wp-content/uploads/2017/09/3-1.png\" alt> 然后我们开始编写parse方法:</p>\n<pre><code>def parsePage(til,text):\n    try:\n        r1=re.compile(r&apos;&quot;view_price&quot;:&quot;[\\d.]*&quot;&apos;)\n        r2=re.compile(r&apos;&quot;raw_title&quot;:&quot;.*?&quot;&apos;)\n        lip=re.findall(r1,text)\n        lit=re.findall(r2,text)\n        for i in range(len(lip)):\n            price=eval(lip[i].split(&apos;:&apos;)[1])\n            title=eval(lit[i].split(&apos;:&apos;)[1])\n            til.append([price,title])\n    except:\n        print(&apos;&apos;)</code></pre><p>其中eval可以忽略””这玩意？？？ 最终完成品:</p>\n<pre><code>#-*-coding:utf-8-*-\nimport requests\nimport re\n\ndef getHTMLtext(url):\n    try:\n        r=requests.get(url)\n        r.raise_for_status()\n        r.encoding=r.apparent_encoding\n        return r.text\n    except:\n        print(&quot;&quot;)\n\n\ndef parsePage(til,text):\n    try:\n        r1=re.compile(r&apos;&quot;view_price&quot;:&quot;[\\d.]*&quot;&apos;)\n        r2=re.compile(r&apos;&quot;raw_title&quot;:&quot;.*?&quot;&apos;)\n        lip=re.findall(r1,text)\n        lit=re.findall(r2,text)\n        for i in range(len(lip)):\n            price=eval(lip[i].split(&apos;:&apos;)[1])\n            title=eval(lit[i].split(&apos;:&apos;)[1])\n            til.append([price,title])\n    except:\n        print(&apos;&apos;)\n\ndef printResult(lis):\n    tplt=&quot;{:4}\\t{:8}\\t{:16}&quot;\n    print(tplt.format(&apos;序号&apos;,&apos;价格&apos;,&apos;商品名称&apos;))\n    for i in range(len(lis)):\n        print(tplt.format(i+1,lis[i][0],lis[i][1]))\n        if lis[i][0]==&apos;89.00&apos;:\n            print(&apos;===============&apos;)\n\ndef main():\n    goods=&apos;书包&apos;\n    deepth=3\n    infolist=[]\n    start_url=&apos;https://s.taobao.com/search?q=书包&apos;\n    for i in range(deepth):\n        try:\n            url=start_url+&apos;&amp;s=&apos;+str(i*44)\n            text=getHTMLtext(url)\n            parsePage(infolist,text)\n        except:\n            continue\n    printResult(infolist)\n\nmain()</code></pre><p>结果: <img src=\"http://be-sunshine.cn/wp-content/uploads/2017/09/4-1.png\" alt></p>\n","text":"首先我们先观察搜索为书包的链接: 第一页: https://s.taobao.com/search?q=书包&amp;imgfile=&amp;commend=all&amp;ssid=s5-e&amp;search_type=item&amp;sourceId=tb.index","link":"","raw":null,"photos":[],"categories":[{"name":"Python","slug":"Python","count":41,"path":"api/categories/Python.json"},{"name":"未分类","slug":"Python/未分类","count":3,"path":"api/categories/Python/未分类.json"},{"name":"爬虫","slug":"Python/未分类/爬虫","count":2,"path":"api/categories/Python/未分类/爬虫.json"}],"tags":[{"name":"爬虫","slug":"爬虫","count":4,"path":"api/tags/爬虫.json"},{"name":"UVa","slug":"UVa","count":39,"path":"api/tags/UVa.json"}]}]}