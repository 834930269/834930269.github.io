{"name":"爬虫","slug":"Python/爬虫","count":2,"postlist":[{"title":"Python 爬取中国大学排名","slug":"python-chut","date":"2017-08-30T10:19:46.000Z","updated":"2019-07-03T13:51:36.852Z","comments":true,"path":"api/articles/python-chut.json","excerpt":"","keywords":null,"cover":"http://be-sunshine.cn/wp-content/uploads/2017/08/17f778160b1fc4d8d2521e6846de2cbe.png","content":"<p>首先是爬取的目的地址: :zap:<a href=\"http://www.zuihaodaxue.cn/zuihaodaxuepaiming2016.html\" title=\"2016中国最好大学排名\" target=\"_blank\" rel=\"noopener\">=&gt;2016中国最好大学排名</a> 这里我们使用的是Python的request库和BeautifulSoup库,IDE用的是Anaconda的Spyder,Python version=3.6. 首先我们现在html中搜索清华大学(废话= =). 结果如下: <img src=\"http://be-sunshine.cn/wp-content/uploads/2017/08/17f778160b1fc4d8d2521e6846de2cbe.png\" alt=\"结果\"> 我们发现结果是在节点为’tbody’中,全部代码中也只有这一个地方有’tbody’.所以我们可以通过查找这个标签,然后再向下查找孩子来找到结果,但是我们知道,他的孩子可能会出现字符串类型,所以我们需要用isinstance(x,bs4.element.Tag),Tag–(标签)来过滤掉其它类型. 于是我们写出A程序:</p>\n<pre><code>#CrawUnivRankingA.py\nimport requests\nfrom bs4 import BeautifulSoup\nimport bs4\n\ndef getHTMLText(url):\n    try:\n        r = requests.get(url, timeout=30)\n        r.raise_for_status()\n        r.encoding = r.apparent_encoding\n        return r.text\n    except:\n        return &quot;&quot;\n\ndef fillUnivList(ulist, html):\n    soup = BeautifulSoup(html, &quot;html.parser&quot;)\n    for tr in soup.find(&apos;tbody&apos;).children:\n        if isinstance(tr, bs4.element.Tag):\n            tds = tr(&apos;td&apos;)\n            ulist.append([tds[0].string, tds[1].string, tds[3].string])\n\ndef printUnivList(ulist, num):\n    print(&quot;{:^10}\\t{:^6}\\t{:^10}&quot;.format(&quot;排名&quot;,&quot;学校名称&quot;,&quot;总分&quot;))\n    for i in range(num):\n        u=ulist[i]\n        print(&quot;{:^10}\\t{:^6}\\t{:^10}&quot;.format(u[0],u[1],u[2]))\n\ndef main():\n    uinfo = []\n    url = &apos;http://www.zuihaodaxue.cn/zuihaodaxuepaiming2016.html&apos;\n    html = getHTMLText(url)\n    fillUnivList(uinfo, html)\n    printUnivList(uinfo, 20) # 20 univs\nmain()</code></pre><p>打印结果: <img src=\"http://be-sunshine.cn/wp-content/uploads/2017/08/807addc82e8f147efd10ff14afadcc03.png\" alt> 可以看出中间的学校缩进不规范,这是因为中文的问题,他是默认按照英文补充空格的. 改进一下,空格用chr(12288)补充:</p>\n<pre><code>#CrawUnivRankingB.py\nimport requests\nfrom bs4 import BeautifulSoup\nimport bs4\n\ndef getHTMLText(url):\n    try:\n        r = requests.get(url, timeout=30)\n        r.raise_for_status()\n        r.encoding = r.apparent_encoding\n        return r.text\n    except:\n        return &quot;&quot;\n\ndef fillUnivList(ulist, html):\n    soup = BeautifulSoup(html, &quot;html.parser&quot;)\n    for tr in soup.find(&apos;tbody&apos;).children:\n        if isinstance(tr, bs4.element.Tag):\n            tds = tr(&apos;td&apos;)\n            ulist.append([tds[0].string, tds[1].string, tds[3].string])\n\ndef printUnivList(ulist, num):\n    tplt = &quot;{0:^10}\\t{1:{3}^10}\\t{2:^10}&quot;\n    print(tplt.format(&quot;排名&quot;,&quot;学校名称&quot;,&quot;总分&quot;,chr(12288)))\n    for i in range(num):\n        u=ulist[i]\n        print(tplt.format(u[0],u[1],u[2],chr(12288)))\n\ndef main():\n    uinfo = []\n    url = &apos;http://www.zuihaodaxue.cn/zuihaodaxuepaiming2016.html&apos;\n    html = getHTMLText(url)\n    fillUnivList(uinfo, html)\n    printUnivList(uinfo, 20) # 20 univs\nmain()</code></pre><p>打印结果: <img src=\"http://be-sunshine.cn/wp-content/uploads/2017/08/a8ee56c437ee19d41e7e53a863779b97.png\" alt> 结束.</p>\n","text":"首先是爬取的目的地址: :zap:=&gt;2016中国最好大学排名 这里我们使用的是Python的request库和BeautifulSoup库,IDE用的是Anaconda的Spyder,Python version=3.6. 首先我们现在html中搜索清华大学(废话= =)","link":"","raw":null,"photos":[],"categories":[{"name":"Python","slug":"Python","count":41,"path":"api/categories/Python.json"},{"name":"爬虫","slug":"Python/爬虫","count":2,"path":"api/categories/Python/爬虫.json"}],"tags":[{"name":"Python","slug":"Python","count":65,"path":"api/tags/Python.json"},{"name":"爬虫","slug":"爬虫","count":4,"path":"api/tags/爬虫.json"}]},{"title":"词云: Python爬取国际时事","slug":"wordcloud-python","date":"2018-04-01T11:30:24.000Z","updated":"2019-07-03T13:51:36.863Z","comments":true,"path":"api/articles/wordcloud-python.json","excerpt":"","keywords":null,"cover":"http://be-sunshine.cn/wp-content/uploads/2018/04/QQ%E6%88%AA%E5%9B%BE20180401190628.jpg","content":"<h1><span id=\"前置工具\">前置工具</span></h1><blockquote>\n<p>python wordcloud jieba BeautifulSoup matplotlib scipy</p>\n</blockquote>\n<h1><span id=\"第一步-爬取国际时事列表\">第一步: 爬取国际时事列表</span></h1><h2><span id=\"待爬地址-httpmsohucomcr57page1ampv2\">待爬地址: </span></h2><blockquote>\n<p>首先我们可以观察到,每点击列表中的下一页时, page 会加一</p>\n<blockquote>\n<p>然后我们就可以确认,想获取多少条信息,直接替换page属性的值即可</p>\n</blockquote>\n<p>然后我们观察想要爬取的内容:</p>\n</blockquote>\n<p><img src=\"http://be-sunshine.cn/wp-content/uploads/2018/04/QQ%E6%88%AA%E5%9B%BE20180401190628.jpg\" alt></p>\n<h2><span id=\"审查元素\">审查元素:</span></h2><p><img src=\"http://be-sunshine.cn/wp-content/uploads/2018/04/QQ%E6%88%AA%E5%9B%BE20180401190805.jpg\" alt></p>\n<blockquote>\n<p>我们发现文本都是在 div(class=”bd3 pb1”) -&gt; div -&gt; p -&gt; a 标签下的:</p>\n</blockquote>\n<h2><span id=\"编写代码\">编写代码</span></h2><blockquote>\n<p>爬取数据并保存在<strong>data.txt</strong>中:</p>\n</blockquote>\n<pre><code># coding: utf-8\n\nfrom wordcloud import WordCloud\nimport requests\nfrom bs4 import BeautifulSoup\nimport re\n\ndef getHTMLText(url):\n    try:\n        r = requests.get(url)\n        r.raise_for_status()\n        r.encoding = r.apparent_encoding\n        return r.text\n    except:\n        pass\n\ndef has_p_a(tag):\n    pass\n\ndef getWannaData(stockURL,res):\n    html = getHTMLText(stockURL)\n    soup = BeautifulSoup(html,&apos;html.parser&apos;)\n    p = soup.find(&apos;div&apos;,class_=&quot;bd3 pb1&quot;).find_all(&apos;a&apos;)\n    for q in p:\n        res.append(q.text)\n\nres = []\nmaxn = 100\nfor i in range(1,maxn):\n    getWannaData(&apos;http://m.sohu.com/cr/57/?page=&apos;+str(i)+&apos;&amp;v=2&apos;,res)\n\nfile = open(&apos;data.txt&apos;,&apos;a+&apos;)\nfor q in res:\n    file.write(q+&apos;\\n&apos;)</code></pre><blockquote>\n<p>其中maxn是控制爬取多少页的</p>\n</blockquote>\n<h2><span id=\"datatxt-部分内容\">data.txt 部分内容:</span></h2><p><img src=\"http://be-sunshine.cn/wp-content/uploads/2018/04/QQ%E6%88%AA%E5%9B%BE20180401191238.jpg\" alt></p>\n<h1><span id=\"第二步-生成词云\">第二步: 生成词云</span></h1><h2><span id=\"前置\">前置</span></h2><blockquote>\n<p>因为要进行中文分词,所以要用jieba 注意再打开<strong>data.txt</strong>时<strong>编码</strong>问题 还有ttf不能保存在有中文的路径下</p>\n</blockquote>\n<h2><span id=\"背景图片\">背景图片</span></h2><blockquote>\n<p>我们选择 <strong>水伊布.png</strong></p>\n</blockquote>\n<p><img src=\"http://be-sunshine.cn/wp-content/uploads/2018/04/timg.jpg\" alt></p>\n<h2><span id=\"生成词云\">生成词云</span></h2><p><img src=\"http://be-sunshine.cn/wp-content/uploads/2018/04/yb.png\" alt> <strong>容我说一句,在中国相对封闭的网络环境中,已经可以看到世界如此的乱了,全部的词条大部分是消极的…看起来大规模战争结束的时间太久了…(还是说,世界就没有安宁过)</strong></p>\n<blockquote>\n<p>这张图可以找到安倍</p>\n</blockquote>\n<p><img src=\"http://be-sunshine.cn/wp-content/uploads/2018/04/yb2.png\" alt></p>\n","text":"前置工具python wordcloud jieba BeautifulSoup matplotlib scipy第一步: 爬取国际时事列表待爬地址: 首先我们可以观察到,每点击列表中的下一页时, page 会加一然后我们就可以确认,想获取多少条信息,直接替换page属性的值即可","link":"","raw":null,"photos":[],"categories":[{"name":"Python","slug":"Python","count":41,"path":"api/categories/Python.json"},{"name":"爬虫","slug":"Python/爬虫","count":2,"path":"api/categories/Python/爬虫.json"}],"tags":[{"name":"Python","slug":"Python","count":65,"path":"api/tags/Python.json"},{"name":"爬虫","slug":"爬虫","count":4,"path":"api/tags/爬虫.json"}]}]}