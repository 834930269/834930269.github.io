{"name":"聊天机器人内核","slug":"Python/聊天机器人内核","count":1,"postlist":[{"title":"聊天室内核从0开始 – 2 处理语料库","slug":"type-3","date":"2019-01-04T12:48:01.000Z","updated":"2019-07-03T13:51:36.863Z","comments":true,"path":"api/articles/type-3.json","excerpt":"","keywords":null,"cover":null,"content":"<blockquote>\n<p>最后更新于2019/1/4</p>\n</blockquote>\n<h1><span id=\"处理语料库\">处理语料库</span></h1><h2><span id=\"简单处理提问与应答\">简单处理提问与应答</span></h2><blockquote>\n<p>这里我们本来可以用jieba或者其他的一些库来帮忙分词,但是因为网上存在已经分好词的语料库,所以可以省略这一步. 至于语料库,可以在github上直接搜索语料库即可以搜到. 一般语料库如下:</p>\n</blockquote>\n<pre><code>E\nM 呵/呵\nM 是/王/若/猫/的/。\nE\nM 不/是\nM 那/是/什/么/？\nE\nM 怎/么/了\nM 我/很/难/过/，/安/慰/我/~\nE\nM 开/心/点/哈/,/一/切/都/会/好/起/来\nM 嗯/ /会/的</code></pre><p>其中E代表是下一组应答的开始,M代表的是一句话. 可以认为是一问一答.</p>\n<h2><span id=\"将一个句子编码化\">将一个句子编码化</span></h2><blockquote>\n<p>因为我们如果企图对一个句子进行判断和操作时,我们需要将这串句子编码化成为一组数字更为方便,且占内存更少. 当然,我们用这组句子编码成数字以后也可以重新根据数字编码回字符.</p>\n</blockquote>\n<p>举个栗子: map[3]=’a’,map[2]=’b’ 则 32 = ab,且 ba = 23</p>\n<blockquote>\n<p>Python的一个语法糖:</p>\n<blockquote>\n<p>[2]*2=[2,2]</p>\n</blockquote>\n<p>2019/1/9 日更新-完善注释</p>\n</blockquote>\n<pre><code>import numpy as np\n\n# 句子编码化\nclass WordSequence(object):\n    #标注TAG\n    PAD_TAG=&apos;&lt;pad&gt;&apos;\n    UNK_TAG=&apos;&lt;unk&gt;&apos; # 未识别\n    START_TAG=&apos;&lt;s&gt;&apos;\n    END_TAG=&apos;&lt;/s&gt;&apos;\n\n    PAD=0\n    UNK=1\n    START=2\n    END=3\n\n    # 初始化标签\n    def __init__(self):\n        self.dict = {\n            WordSequence.PAD_TAG: WordSequence.PAD,\n            WordSequence.UNK_TAG: WordSequence.UNK,\n            WordSequence.START_TAG: WordSequence.START,\n            WordSequence.END_TAG: WordSequence.END\n        }\n        # 是否训练过了\n        self.fited=False\n\n    # 将word的词性转换为下标\n    def to_index(self,word):\n        assert self.fited,&apos;WordSequence尚未进行fit操作&apos;\n        # 如果有,返回下标\n        if word in self.dict:\n            return self.dict[word]\n        # 没有,返回UNKnow\n        return WordSequence.UNK\n\n    def to_word(self,index):\n        assert self.fited,&apos;WordSequence尚未进行fit操作&apos;\n        # 遍历dict,找到就返回value\n        for k,v in self.dict.items():\n            if v==index:\n                return k\n        # 否则返回不知道\n        return WordSequence.UNK_TAG\n\n    def size(self):\n        assert self.fited, &apos;WordSequence尚未进行fit操作&apos;\n        return len(self.dict) + 1\n\n    def __len__(self):\n        return self.size()\n\n    # 对数据进行处理\n    def fit(self,sentences,min_count=5,max_count=None,max_features=None):\n        assert not self.fited, &apos;WordSequence只能进行一次fit&apos;\n\n        count={}\n        # 遍历所有句子\n        for sentence in sentences:\n            arr=list(sentence)\n            # 统计词频\n            for a in arr:\n                if a not in count:\n                    count[a]=0\n                count[a]+=1\n        # 只统计词频大于最小值的\n        if min_count is not None:\n            count={k:v for k,v in count.items() if v&gt;=min_count}\n        # 仅统计词频小于最大值的\n        if max_count is not None:\n            count={k:v for k,v in count.items() if v&lt;=max_count}\n\n        # 如果有特征值数的限制,比如[1,2,3]max_features=2,则\n        # 需要用的是[1,2]\n        if isinstance(max_features,int):\n            # list(dict)=[(key,value),...]\n            count = sorted(list(count.items()),key=lambda x:x[1])\n            if max_features is not None and len(count) &gt; max_features:\n                count = count[-int(max_features):]# 从尾部向前\n            # 这个以及下面那个类似于前向星式存图法里的\n            # 下标递增式存法,即加入一个元素,该元素下标\n            # 变成当前已存在的元素个数\n            # 就是把count中的key作为dict的key\n            # 在dict中的下标作为dict的value\n            for w,_ in count:\n                self.dict[w]=len(self.dict)\n        else:\n            for w in sorted(count.keys()):\n                self.dict[w]=len(self.dict)\n\n        # 处理完成\n        self.fited=True\n\n    # 序列成数列\n    def transform(self, sentence, max_len=None):\n        assert self.fited, &apos;WordSequence尚未进行fit操作&apos;\n        # PAD -&gt; 填充标签,先填充本来的句子长度所有元素为PAD\n        # 如: [&apos;&lt;PAD&gt;&apos;,&apos;&lt;PAD&gt;&apos;...]\n        if max_len is not None:\n            r = [self.PAD] * max_len\n        else:\n            r = [self.PAD] * len(sentence)\n\n        for index, a in enumerate(sentence):\n            if max_len is not None and index &gt;= len(r):\n                break\n            r[index] = self.to_index(a)\n\n        return np.array(r)\n\n    # 序列转回字母\n    def inverse_transform(self, indices, ignore_pad=False, ignore_unk=False, ignore_start=False, ignore_end=False):\n        ret = []\n        for i in indices:\n            word = self.to_word(i)\n            if word == WordSequence.PAD_TAG and ignore_pad:\n                continue\n            if word == WordSequence.UNK_TAG and ignore_unk:\n                continue\n            if word == WordSequence.START_TAG and ignore_start:\n                continue\n            if word == WordSequence.END_TAG and ignore_end:\n                continue\n\n            ret.append(word)\n\n        return ret\n\n\ndef test():\n    ws = WordSequence()\n    ws.fit([[&apos;你&apos;, &apos;好&apos;, &apos;啊&apos;], [&apos;你&apos;, &apos;好&apos;, &apos;哦&apos;], ])\n\n    indice = ws.transform([&apos;我&apos;, &apos;们&apos;, &apos;好&apos;])\n    print(indice)\n\n    back = ws.inverse_transform(indice)\n    print(back)\n\n\nif __name__ == &apos;__main__&apos;:\n    test()</code></pre><blockquote>\n<p>可以发现其实都是一些简单的映射.将每个字符都映射到一个整数上面去.</p>\n<blockquote>\n<p>这样做以后再将其打包成pkl会大大减少占用硬盘: 83MB-&gt;750kb 了解一下</p>\n</blockquote>\n</blockquote>\n<h2><span id=\"对于语料中句子的规范化\">对于语料中句子的规范化</span></h2><blockquote>\n<p>这里我们提供三个函数(可自行编码):</p>\n</blockquote>\n<pre><code># 这个函数的作用是在有多个回答的条件下将回答合并起来\ndef make_split(line):\n    if re.match(r&apos;.*([，···?!\\.,!？])$&apos;, &apos;&apos;.join(line)):\n        return []\n\n    return [&apos;, &apos;]\n\n# 是否是一个有意义的句子(这里我们不做规则)\ndef good_line(line):\n    #if len(re.findall(r&apos;[a-zA-Z0-9]&apos;, &apos;&apos;.join(line))) &gt; 2:\n        #return False\n    return True\n\n# 规范化语料,即对于已提取出的预料中的句子进行处理\ndef regular(sen):\n    #sen = re.sub(r&apos;\\.{3,100}&apos;, &apos;···&apos;, sen)\n    #sen = re.sub(r&apos;···{2,100}&apos;, &apos;···&apos;, sen)\n    #sen = re.sub(r&apos;[,]{1,100}&apos;, &apos;，&apos;, sen)\n    #sen = re.sub(r&apos;[\\.]{1,100}&apos;, &apos;。&apos;, sen)\n    #sen = re.sub(r&apos;[\\?]{1,100}&apos;, &apos;？&apos;, sen)\n    #sen = re.sub(r&apos;[!]{1,100}&apos;, &apos;！&apos;, sen)\n\n    return sen</code></pre><blockquote>\n<p>可以发现我基本都注释掉了,因为小黄鸡的语料库就是标准的一问一答,不是自然地语料库(微信随便提取的那类),所以不需要过多的处理.</p>\n</blockquote>\n<h2><span id=\"打包成pkl文件\">打包成pkl文件</span></h2><blockquote>\n<p>首先说一下pkl文件:</p>\n<blockquote>\n<p>pkl文件是Python运行时产生的数据序列化后存储下来的文件格式,类似于其他语言的序列化.方便以后的继续使用和读取.</p>\n</blockquote>\n</blockquote>\n<p>具体的处理语料库和打包(一问一答Tuple)代码如下:</p>\n<pre><code># -*- coding:utf-8 -*-\n\nimport re\nimport pickle\nimport sys\nfrom tqdm import tqdm\n\n\ndef make_split(line):\n    if re.match(r&apos;.*([，···?!\\.,!？])$&apos;, &apos;&apos;.join(line)):\n        return []\n\n    return [&apos;, &apos;]\n\n# 是否是一个有意义的句子(这里我们不做规则)\ndef good_line(line):\n    #if len(re.findall(r&apos;[a-zA-Z0-9]&apos;, &apos;&apos;.join(line))) &gt; 2:\n        #return False\n    return True\n\n# 规范化语料\ndef regular(sen):\n    #sen = re.sub(r&apos;\\.{3,100}&apos;, &apos;···&apos;, sen)\n    #sen = re.sub(r&apos;···{2,100}&apos;, &apos;···&apos;, sen)\n    #sen = re.sub(r&apos;[,]{1,100}&apos;, &apos;，&apos;, sen)\n    #sen = re.sub(r&apos;[\\.]{1,100}&apos;, &apos;。&apos;, sen)\n    #sen = re.sub(r&apos;[\\?]{1,100}&apos;, &apos;？&apos;, sen)\n    #sen = re.sub(r&apos;[!]{1,100}&apos;, &apos;！&apos;, sen)\n\n    return sen\n\n# 这样设置的意思是无限制\ndef main(limit=99999, x_limit=1, y_limit=1):\n    from word_sequence import WordSequence\n    print(&apos;extract lines&apos;)\n    fp=open(&quot;xiaohuangji.conv&quot;,&apos;r&apos;,errors=&apos;ignore&apos;,encoding=&apos;utf-8&apos;)\n\n    groups=[]\n    group=[]\n\n    # 提取出所有问答组\n    for line in tqdm(fp):\n        if line.startswith(&apos;M &apos;):\n            line=line.replace(&apos;\\n&apos;,&apos;&apos;)\n            if &apos;/&apos; in line:\n                line = line[2:].split(&apos;/&apos;)\n            else:\n                line=list(line[2:])\n            line=line[:-1]\n\n            group.append(list(regular(&apos;&apos;.join(line))))\n        else:\n            if group:\n                groups.append(group)\n                group = []\n    if group:\n        groups.append(group)\n        group = []\n\n    print(&apos;extract group&apos;)\n\n    x_data = []\n    y_data = []\n    # 将问与答分开\n    for group in tqdm(groups):\n        for i,line in  enumerate(group):\n            last_line=None\n            # last_line是上一句\n            if i&gt;0:\n                last_line = group[i-1]\n                if not good_line(last_line):\n                    last_line = None\n            if i&lt;len(group)-1:\n                next_line=group[i+1]\n                if not good_line(next_line):\n                    next_line=None\n            # 如果有下一句\n            if not last_line:\n                x_data.append(line)\n                y_data.append(next_line)\n\n        #print(len(x_data), len(y_data))\n\n    print(len(x_data), len(y_data))\n    # 构建问答,测试前20个\n    for ask,answer in zip(x_data[:20],y_data[:20]):\n        print(&apos;&apos;.join(ask))\n        print(&apos;&apos;.join(answer))\n        print(&apos;-&apos; * 20)\n\n    # 生成pkl文件\n    data=list(zip(x_data,y_data))\n\n    data=[\n        (x,y) for x,y in data if limit&gt;len(x) &gt;=x_limit and limit &gt; len(y) &gt;= y_limit\n    ]\n\n    # 打包成pkl\n    x_data, y_data = zip(*data)\n    ws_input = WordSequence()\n    ws_input.fit(x_data + y_data)\n    print(&apos;dump&apos;)\n    pickle.dump(\n        (x_data, y_data), open(&apos;chatbot.pkl&apos;, &apos;wb&apos;))\n    pickle.dump(ws_input, open(&apos;ws.pkl&apos;, &apos;wb&apos;))\n    print(&apos;done&apos;)\n\nif __name__ == &apos;__main__&apos;:\n    main()</code></pre><h1><span id=\"聊天室内核从0开始-3-seq2seq\">聊天室内核从0开始 – 3 Seq2Seq</span></h1><p><a href=\"http://be-sunshine.cn/index.php/2019/01/06/seq2seq/\" title=\"聊天室内核从0开始 – 3 Seq2Seq\" target=\"_blank\" rel=\"noopener\">聊天室内核从0开始 – 3 Seq2Seq</a></p>\n","text":"最后更新于2019/1/4处理语料库简单处理提问与应答这里我们本来可以用jieba或者其他的一些库来帮忙分词,但是因为网上存在已经分好词的语料库,所以可以省略这一步. 至于语料库,可以在github上直接搜索语料库即可以搜到. 一般语料库如下:EM 呵/呵M 是/王/若/猫/的/","link":"","raw":null,"photos":[],"categories":[{"name":"Python","slug":"Python","count":41,"path":"api/categories/Python.json"},{"name":"聊天机器人内核","slug":"Python/聊天机器人内核","count":1,"path":"api/categories/Python/聊天机器人内核.json"}],"tags":[{"name":"Python","slug":"Python","count":65,"path":"api/tags/Python.json"},{"name":"聊天机器人内核","slug":"聊天机器人内核","count":3,"path":"api/tags/聊天机器人内核.json"}]}]}