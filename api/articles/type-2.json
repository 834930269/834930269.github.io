{"title":"聊天室内核从0开始 - 1 前置知识与NLP","slug":"type-2","date":"2019-01-01T13:02:42.000Z","updated":"2019-07-03T13:51:36.863Z","comments":true,"path":"api/articles/type-2.json","photos":[],"link":"","excerpt":null,"covers":["http://be-sunshine.cn/wp-content/uploads/2019/01/1234352-13d969531284a9f9.png","http://be-sunshine.cn/wp-content/uploads/2019/01/20180507224752660.png","http://be-sunshine.cn/wp-content/uploads/2019/01/p.png","http://be-sunshine.cn/wp-content/uploads/2019/01/1042406-20170306142253375-175971779.png","http://be-sunshine.cn/wp-content/uploads/2019/01/20180719232051968.jpg"],"content":"<blockquote>\n<p>最后更新于2019/1/6</p>\n</blockquote>\n<h1><span id=\"前置知识\">前置知识</span></h1><h2><span id=\"tensorflow\">TensorFlow</span></h2><blockquote>\n<ol>\n<li>张量(Tensor)</li>\n<li>图(Flow-&gt;Graph)</li>\n<li>会话(Session)</li>\n</ol>\n</blockquote>\n<h3><span id=\"张量tensor\">张量(Tensor)</span></h3><blockquote>\n<p>类似于矩阵,一维的张量叫做向量</p>\n</blockquote>\n<h3><span id=\"计算图graph\">计算图(Graph)</span></h3><blockquote>\n<p>TensorFlow的计算图的组成和数据结构中的图不同.</p>\n<blockquote>\n<ol>\n<li>图的节点: op-&gt;即操作.</li>\n<li>图的边: 即数据流,此处的数据流就是上述张量.</li>\n</ol>\n</blockquote>\n</blockquote>\n<h3><span id=\"会话session\">会话(Session)</span></h3><blockquote>\n<p>在TensorFlow中,要想启动一个图的前提是要先创建一个会话,Ts所有对图的操作,都必须在会话中进行.</p>\n</blockquote>\n<h3><span id=\"模型训练一般流程\">模型训练一般流程</span></h3><p>document.write(“graph TD\\nA(开始) –&gt; B(定义数据集)\\nB –&gt; C(定义模型)\\nC –&gt;D(编写并训练模型)\\nD –&gt;E(模型测试)\\n”);</p>\n<h2><span id=\"android\">Android</span></h2><h3><span id=\"模拟机替代avg\">模拟机替代AVG</span></h3><p>为了解决Android Studio使用AVG虚拟机时会导致内存占用过大的问题. 这里我们选用网易旗下的MuMu模拟器.</p>\n<h4><span id=\"步骤\">步骤</span></h4><p>document.write(“graph TD\\nA(开启MuMu模拟器) –&gt; B(进入到MuMu根目录的上层目录)\\nB –&gt; C(找到vmonitorbin)\\nC –&gt;D(在cmd中进入到上述地址-先进入盘符)\\nD –&gt;E(输入adb_server.exe connect port-如7555)\\n”);</p>\n<p>注意:这个必须在Android Studio开启时连接才可以,如果出现Empty host name,多连接几次就可以了. 使用这个方法就可以有效减少内存的占用问题.</p>\n<h2><span id=\"神经网络\">神经网络</span></h2><h3><span id=\"神经网络相关学习视频\">神经网络相关学习视频</span></h3><p><a href=\"https://www.icourse163.org/learn/DA-1002183004?tid=1002301011#/learn/content\" target=\"_blank\" rel=\"noopener\">https://www.icourse163.org/learn/DA-1002183004?tid=1002301011#/learn/content</a> 因为其中涉及内容过多,且基础,故我在此仅讲解几个比较生疏难懂的概念,而不做展开,其基础推荐在学号高数和概率论以及线性代数这三门课程以后再进行展开(也可以不需要,但理解上会有些困难).</p>\n<h3><span id=\"梯度下降\">梯度下降</span></h3><blockquote>\n<p>在了解梯度下降前你需要知道</p>\n<blockquote>\n<ol>\n<li>计算图的节点是简单的操作</li>\n<li>高数求导中的链式求导法</li>\n<li>既然它们的节点时简单的运算,那么就可以很方便地使用链式求导法则对其进行求导 &gt; 比如 Y=Z+b,Z=b+2 dY/db=dZ/db+d(b)/db</li>\n</ol>\n</blockquote>\n<p>则如下图所示 <img src=\"http://be-sunshine.cn/wp-content/uploads/2019/01/1234352-13d969531284a9f9.png\" alt> 计算图是正向传播的,而计算图中的反向计算梯度是反向计算的,即函数的上升方向. 故反向传播以后计算出来的梯度只需要取负即是梯度下降的方向了.</p>\n</blockquote>\n<p>以上基础知识熟悉以后可以看这篇文章: <a href=\"https://www.jianshu.com/p/c7e642877b0e\" title=\"深入浅出,梯度下降法及其实现\" target=\"_blank\" rel=\"noopener\">深入浅出,梯度下降法及其实现</a></p>\n<h3><span id=\"向量化\">向量化</span></h3><blockquote>\n<p>这个不难理解,即计算图中数据流都是向量,大大缩短计算时间.</p>\n</blockquote>\n<h3><span id=\"python广播机制\">Python广播机制</span></h3><blockquote>\n<p>举几个例子吧,具体在实践中总结,或查阅相关DOC.</p>\n</blockquote>\n<pre><code>A = numpy.array([1,2,3])\nresult = A + 100\nprint(result)\n\n输出: [101 102 103]</code></pre><h3><span id=\"激活函数\">激活函数</span></h3><blockquote>\n<p>计算图的每层节点将上一层的输出作为本层的输入,如果没有激活函数,那么最终的结果等效于F(x)=x即线性函数. 而激活函数则是在层与层间添加上一个激活函数,使之并不完全作为线性函数来传递数据(即进入什么数据,出来就一定是唯一结果) 即由激活函数判断是否输出</p>\n</blockquote>\n<h3><span id=\"神经元模型参考书籍-西瓜书\">神经元模型(参考书籍-西瓜书)</span></h3><p><img src=\"http://be-sunshine.cn/wp-content/uploads/2019/01/20180507224752660.png\" alt></p>\n<blockquote>\n<p>在生物神经网络中,，每个神经元与其他神经元相连,当它”兴奋”时,就会向相连的神经元发送化学物质,从而改变这些神经元内的电位,如果某神经元的电位超过一个”阈值”,那么他就会被激活,即”兴奋”起来,向其他神经元发送化学物质. 现在最常用的组成神经网络的节点神经元模型是M-P神经元模型.在这个模型中,神经元接收到来自其它n个神经元传递过来的输入信号,这些输入信号通过带权重的连接进行传递,神经元接收到的总输入值将与神经元的阈值进行比较,然后通过”激活函数”处理以产生神经元的输出.</p>\n</blockquote>\n<h3><span id=\"神经网络分类\">神经网络分类</span></h3><blockquote>\n<p>大部分在上述的视频中都有系统介绍,这里我只做总结</p>\n</blockquote>\n<h4><span id=\"感知机与深层神经网络\">感知机与深层神经网络</span></h4><p><img src=\"http://be-sunshine.cn/wp-content/uploads/2019/01/p.png\" alt></p>\n<blockquote>\n<p>即 输入层 -&gt; 隐藏层 -&gt; 输出层</p>\n</blockquote>\n<h4><span id=\"bp神经网络\">BP神经网络</span></h4><blockquote>\n<p>BP网络即前向传播+反向传播来更新偏置. 特点:</p>\n<blockquote>\n<p>1-可以通过逐层信息传递到最后的输出. 2-沿着一条直线计算,直到最后一层,求出计算结果. 3-包含输入层、输出层和隐藏层,其目的是实现从输入到输出的映射. 4-一般包含多层,并且层与层之间是全连接的,不存在同层和跨层连接.</p>\n</blockquote>\n</blockquote>\n<h4><span id=\"循环神经网络rnn和lstm\">循环神经网络RNN和LSTM</span></h4><blockquote>\n<p>这类计算图是针对于成序列的数据的.</p>\n<blockquote>\n<p>类似于造句,造音乐等.如果一个序列过长,则会导致可能在计算后面序列的时候将前面序列的影响变低.从而导致序列无法有效处理”长期依赖”的问题.</p>\n</blockquote>\n</blockquote>\n<h5><span id=\"rnn\">RNN</span></h5><h6><span id=\"前向传播\">前向传播</span></h6><blockquote>\n<p>普通的CNN模型即上述的神经网络模型,而NLP遵守的规则一般为对于一个句子的分析,每个单词的分析,如果采取上述的线性模型可能会导致语言没有一点逻辑,即 “我/爱/你” - 则针对我输出A,针对爱输出B,针对你输出C 得到结果ABC. 而RNN最大的特点在于其可记忆性.什么叫可记忆性呢?即上一个单词的结果要传递给下一层,使下一个单词运算出的结果可以结合上一个单词以及当前的单词一起得出最后的结果.</p>\n</blockquote>\n<p><img src=\"http://be-sunshine.cn/wp-content/uploads/2019/01/1042406-20170306142253375-175971779.png\" alt></p>\n<blockquote>\n<p>如上图,右侧是拆分后的RNN,x(i)代表的是第i个单词,而计算后的h(i)会传递给h(i+1),y(i)是由上一层计算出来的词向量. 数学公式表达为:</p>\n<blockquote>\n<p>h(i)=g(w*h(i-1)+w*x(i)+bh) g(激活函数)一般选为tanh/Relu y(i)=g(w*h(i)+by) g一般选择sigmod/softmax</p>\n</blockquote>\n<p>优化: 因为词向量的行是相同的,所以将列拼在一起即可.</p>\n<blockquote>\n<p>h(i)=g(w*[h(i-1),x(i)]+bh)</p>\n</blockquote>\n</blockquote>\n<h6><span id=\"反向传播\">反向传播</span></h6><blockquote>\n<p>反向传播作用依然是：减少误差,计算lost函数. 用倒数来计算某一个节点队最终结果的影响程度.训练完后,取平均值(大概,这点我没太仔细看).</p>\n</blockquote>\n<p>一个比较通俗易懂的链接: <a href=\"https://blog.csdn.net/shaomingliang499/article/details/50587300\" title=\" 一步一步教你反向传播\" target=\"_blank\" rel=\"noopener\">一步一步教你反向传播</a></p>\n<h6><span id=\"rnn的几种类型\">RNN的几种类型</span></h6><p><img src=\"http://be-sunshine.cn/wp-content/uploads/2019/01/20180719232051968.jpg\" alt></p>\n<h5><span id=\"lstm\">LSTM</span></h5><blockquote>\n<p>再清楚了RNN以后,LSTM其实就是基于RNN的一个变种.</p>\n<blockquote>\n<p>因为RNN实际应用中无法解决长效记忆的问题,所以催生出了LSTM这一模型.放一个简单的视频可以看下.</p>\n</blockquote>\n<p><a href=\"https://www.bilibili.com/video/av15998549?from=search&seid=17651800282007333668\" title=\"什么是 LSTM RNN 循环神经网络 ?\" target=\"_blank\" rel=\"noopener\">什么是 LSTM RNN 循环神经网络 ?</a></p>\n</blockquote>\n<h2><span id=\"nlp\">NLP</span></h2><h3><span id=\"介绍\">介绍</span></h3><blockquote>\n<p>自然语言处理,探索如何处理及运用自然语言,即让电脑懂人类的语言. 包含文本分析、信息检索、词性标注、问答系统等.</p>\n</blockquote>\n<ol>\n<li><p>词法分析</p>\n<blockquote>\n<p>分词技术、词性标注(名词n,形容词a,副词d,人称代词rr,动词v…)、命名实体识别、词义消歧</p>\n</blockquote>\n</li>\n<li><p>句法分析</p>\n</li>\n<li><p>语义分析</p>\n</li>\n</ol>\n<h3><span id=\"分词技术\">分词技术</span></h3><blockquote>\n<p>中科院分词系统(nlpir): <a href=\"http://ictclas.nlpir.org/nlpir/\" title=\"语义分词系统\" target=\"_blank\" rel=\"noopener\">中科院语义分词系统</a></p>\n</blockquote>\n<h3><span id=\"命名实体识别\">命名实体识别</span></h3><blockquote>\n<p>即分词方法. 命名实体识别（Named Entity Recognition，简称NER），又称作“专名识别”，是指识别文本中具有特定意义的实体，主要包括人名、地名、机构名、专有名词等。 一般分为两种方法</p>\n<blockquote>\n<p>基于规则和词典的方法.</p>\n</blockquote>\n</blockquote>\n<p>document.write(“graph TD\\n\\nC(基于统计的方法)\\nC –&gt;D[隐马尔可夫模型]\\nC –&gt;E[较大熵]\\nC –&gt;F[支持向量机]\\nC –&gt;G[条件随机场]\\n”);</p>\n<h3><span id=\"朴素贝叶斯\">朴素贝叶斯</span></h3><p>即概率论中的贝叶斯概型.</p>\n<blockquote>\n<p>在为序列定型中的用法:</p>\n<blockquote>\n<p>如: 分次以后判断每个单词是垃圾邮件的可能性大小,再用朴素贝叶斯计算出该邮件是垃圾邮件的概率.</p>\n</blockquote>\n</blockquote>\n<h3><span id=\"马尔科夫过程\">马尔科夫过程</span></h3><blockquote>\n<p>其实我也没搞懂意义在哪~ (1)独立随机过程为马尔可夫过程。 (2)独立增量过程为马尔可夫过程：没{X(t)，t∈[0，+∞)}为一独立增量过程，且有P(X(0)=x0)=1，x0为常数，则X(t)为马尔可夫过程。 (3)泊松过程为马尔可夫过程。 (4)维纳过程为马尔可夫过程。 (5)质点随机游动过程为马尔可夫过程。 即下一时刻的状态只依赖于上一时刻,而与上一时刻以前无关.</p>\n</blockquote>\n<h3><span id=\"语料的处理方法\">语料的处理方法</span></h3><ol>\n<li>数据清洗(去掉无意义的标签,url,符号等)</li>\n<li>分词、大小写转换、添加句首句尾、词性标注.</li>\n<li>统计词频、抽取文本特征、特征选择、计算特征权重、归一化</li>\n<li>划分训练集、测试集（先分几份,然后7-3划分）</li>\n</ol>\n<h1><span id=\"聊天室内核从0开始-2-处理语料库\">聊天室内核从0开始 – 2 处理语料库</span></h1><p><a href=\"http://be-sunshine.cn/index.php/2019/01/04/type-3/\" target=\"_blank\" rel=\"noopener\">http://be-sunshine.cn/index.php/2019/01/04/type-3/</a></p>\n","categories":[{"name":"Android","slug":"Android","count":5,"path":"api/categories/Android.json"},{"name":"NLP","slug":"Android/NLP","count":1,"path":"api/categories/Android/NLP.json"},{"name":"TensorFlow","slug":"Android/NLP/TensorFlow","count":1,"path":"api/categories/Android/NLP/TensorFlow.json"},{"name":"机器学习","slug":"Android/NLP/TensorFlow/机器学习","count":1,"path":"api/categories/Android/NLP/TensorFlow/机器学习.json"},{"name":"聊天机器人内核","slug":"Android/NLP/TensorFlow/机器学习/聊天机器人内核","count":1,"path":"api/categories/Android/NLP/TensorFlow/机器学习/聊天机器人内核.json"}],"tags":[{"name":"Android","slug":"Android","count":5,"path":"api/tags/Android.json"},{"name":"机器学习","slug":"机器学习","count":4,"path":"api/tags/机器学习.json"},{"name":"聊天机器人内核","slug":"聊天机器人内核","count":3,"path":"api/tags/聊天机器人内核.json"},{"name":"NLP","slug":"NLP","count":1,"path":"api/tags/NLP.json"},{"name":"TensorFlow","slug":"TensorFlow","count":1,"path":"api/tags/TensorFlow.json"}]}