{"title":"词云: Python爬取国际时事","slug":"wordcloud-python","date":"2018-04-01T11:30:24.000Z","updated":"2019-07-03T13:51:36.863Z","comments":true,"path":"api/articles/wordcloud-python.json","photos":[],"link":"","excerpt":null,"covers":["http://be-sunshine.cn/wp-content/uploads/2018/04/QQ%E6%88%AA%E5%9B%BE20180401190628.jpg","http://be-sunshine.cn/wp-content/uploads/2018/04/QQ%E6%88%AA%E5%9B%BE20180401190805.jpg","http://be-sunshine.cn/wp-content/uploads/2018/04/QQ%E6%88%AA%E5%9B%BE20180401191238.jpg","http://be-sunshine.cn/wp-content/uploads/2018/04/timg.jpg","http://be-sunshine.cn/wp-content/uploads/2018/04/yb.png","http://be-sunshine.cn/wp-content/uploads/2018/04/yb2.png"],"content":"<h1><span id=\"前置工具\">前置工具</span></h1><blockquote>\n<p>python wordcloud jieba BeautifulSoup matplotlib scipy</p>\n</blockquote>\n<h1><span id=\"第一步-爬取国际时事列表\">第一步: 爬取国际时事列表</span></h1><h2><span id=\"待爬地址-httpmsohucomcr57page1ampv2\">待爬地址: </span></h2><blockquote>\n<p>首先我们可以观察到,每点击列表中的下一页时, page 会加一</p>\n<blockquote>\n<p>然后我们就可以确认,想获取多少条信息,直接替换page属性的值即可</p>\n</blockquote>\n<p>然后我们观察想要爬取的内容:</p>\n</blockquote>\n<p><img src=\"http://be-sunshine.cn/wp-content/uploads/2018/04/QQ%E6%88%AA%E5%9B%BE20180401190628.jpg\" alt></p>\n<h2><span id=\"审查元素\">审查元素:</span></h2><p><img src=\"http://be-sunshine.cn/wp-content/uploads/2018/04/QQ%E6%88%AA%E5%9B%BE20180401190805.jpg\" alt></p>\n<blockquote>\n<p>我们发现文本都是在 div(class=”bd3 pb1”) -&gt; div -&gt; p -&gt; a 标签下的:</p>\n</blockquote>\n<h2><span id=\"编写代码\">编写代码</span></h2><blockquote>\n<p>爬取数据并保存在<strong>data.txt</strong>中:</p>\n</blockquote>\n<pre><code># coding: utf-8\n\nfrom wordcloud import WordCloud\nimport requests\nfrom bs4 import BeautifulSoup\nimport re\n\ndef getHTMLText(url):\n    try:\n        r = requests.get(url)\n        r.raise_for_status()\n        r.encoding = r.apparent_encoding\n        return r.text\n    except:\n        pass\n\ndef has_p_a(tag):\n    pass\n\ndef getWannaData(stockURL,res):\n    html = getHTMLText(stockURL)\n    soup = BeautifulSoup(html,&apos;html.parser&apos;)\n    p = soup.find(&apos;div&apos;,class_=&quot;bd3 pb1&quot;).find_all(&apos;a&apos;)\n    for q in p:\n        res.append(q.text)\n\nres = []\nmaxn = 100\nfor i in range(1,maxn):\n    getWannaData(&apos;http://m.sohu.com/cr/57/?page=&apos;+str(i)+&apos;&amp;v=2&apos;,res)\n\nfile = open(&apos;data.txt&apos;,&apos;a+&apos;)\nfor q in res:\n    file.write(q+&apos;\\n&apos;)</code></pre><blockquote>\n<p>其中maxn是控制爬取多少页的</p>\n</blockquote>\n<h2><span id=\"datatxt-部分内容\">data.txt 部分内容:</span></h2><p><img src=\"http://be-sunshine.cn/wp-content/uploads/2018/04/QQ%E6%88%AA%E5%9B%BE20180401191238.jpg\" alt></p>\n<h1><span id=\"第二步-生成词云\">第二步: 生成词云</span></h1><h2><span id=\"前置\">前置</span></h2><blockquote>\n<p>因为要进行中文分词,所以要用jieba 注意再打开<strong>data.txt</strong>时<strong>编码</strong>问题 还有ttf不能保存在有中文的路径下</p>\n</blockquote>\n<h2><span id=\"背景图片\">背景图片</span></h2><blockquote>\n<p>我们选择 <strong>水伊布.png</strong></p>\n</blockquote>\n<p><img src=\"http://be-sunshine.cn/wp-content/uploads/2018/04/timg.jpg\" alt></p>\n<h2><span id=\"生成词云\">生成词云</span></h2><p><img src=\"http://be-sunshine.cn/wp-content/uploads/2018/04/yb.png\" alt> <strong>容我说一句,在中国相对封闭的网络环境中,已经可以看到世界如此的乱了,全部的词条大部分是消极的…看起来大规模战争结束的时间太久了…(还是说,世界就没有安宁过)</strong></p>\n<blockquote>\n<p>这张图可以找到安倍</p>\n</blockquote>\n<p><img src=\"http://be-sunshine.cn/wp-content/uploads/2018/04/yb2.png\" alt></p>\n","categories":[{"name":"Python","slug":"Python","count":41,"path":"api/categories/Python.json"},{"name":"爬虫","slug":"Python/爬虫","count":2,"path":"api/categories/Python/爬虫.json"}],"tags":[{"name":"Python","slug":"Python","count":65,"path":"api/tags/Python.json"},{"name":"爬虫","slug":"爬虫","count":4,"path":"api/tags/爬虫.json"}]}