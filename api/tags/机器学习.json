{"name":"机器学习","slug":"机器学习","count":4,"postlist":[{"title":"python 机器学习 科学计算库","slug":"python-sklearn-moudle","date":"2018-01-08T08:34:43.000Z","updated":"2019-07-03T13:51:36.860Z","comments":true,"path":"api/articles/python-sklearn-moudle.json","excerpt":"","keywords":null,"cover":null,"content":"<h1 id=\"Jupyter-Notebook\"><a href=\"#Jupyter-Notebook\" class=\"headerlink\" title=\"Jupyter Notebook\"></a>Jupyter Notebook</h1><p><strong>math Last Checkpoint: a few seconds ago (autosaved) [Python 3]</strong> <strong>Python 3</strong> <strong>Code:</strong></p>\n<pre><code>import numpy as np\n\nnp.arange(10)\n\narray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\na=np.arange(10)\n\n#可以直接对数组进行运算\n\na = a ** 2\n\na\n\narray([ 0,  1,  4,  9, 16, 25, 36, 49, 64, 81], dtype=int32)\n\n​\n\n#Scipy\n\n#用来做高等数学等计算的包\n\nfrom scipy import linalg\n\n#生成一个二维数组\n\nA = np.array([[1,2],[3,4]])\n\nA\n\narray([[1, 2],\n       [3, 4]])\n\n#计算行列式的值\n\n#1*4-2*3\n\nlinalg.det(A)\n\n​\n\n-2.0\n\n#Pandas\n\n#是一种构建于Numpy的高级数据结构和精巧工具,快速简单的处理数据\n\nimport pandas as pd\n\n#序列\n\ns = pd.Series([1,3,5,np.nan,6,8])\n\ns\n\n0    1.0\n1    3.0\n2    5.0\n3    NaN\n4    6.0\n5    8.0\ndtype: float64\n\n#时间数组,起始时间到六天\n\ndates = pd.date_range(&apos;20130101&apos;,periods=6)\n\ndates\n\nDatetimeIndex([&apos;2013-01-01&apos;, &apos;2013-01-02&apos;, &apos;2013-01-03&apos;, &apos;2013-01-04&apos;,\n               &apos;2013-01-05&apos;, &apos;2013-01-06&apos;],\n              dtype=&apos;datetime64[ns]&apos;, freq=&apos;D&apos;)\n\n#生成表格\n\n#index:行标识\n\n#columns:列标识\n\n#rand是0-1的均匀分布，randn是均值为0方差为1的正态分布；\n\n#rand(n)或randn(n)生成n*n的随机数矩阵。\n\n#rand(n,m)或randn(m,n)生成m*n的随机数矩阵。\n\ndf = pd.DataFrame(np.random.randn(6,4),index=dates,columns=list(&apos;ABCD&apos;))\n\ndf\n\n    A   B   C   D\n2013-01-01  1.210884    0.616424    0.961066    0.173936\n2013-01-02  0.358245    0.506724    -0.047834   0.587061\n2013-01-03  -0.508396   0.012049    -0.114224   -1.195929\n2013-01-04  2.303441    0.536666    -1.013810   -0.574154\n2013-01-05  -1.327828   -0.003089   0.662432    0.038886\n2013-01-06  1.379826    1.554135    -0.681174   -0.816094\n\n#通过B列降序排序\n\ndf.sort_values(by=&apos;B&apos;)\n\n#从上到下多少行\n\n#df.head()\n\n#从下到上多少行\n\n#df.tail()\n\n#所有值和描述\n\n#df.describe()\n\n#转置\n\n#df.T\n\n​\n\n    A   B   C   D\n2013-01-01  1.210884    0.616424    0.961066    0.173936\n2013-01-02  0.358245    0.506724    -0.047834   0.587061\n2013-01-03  -0.508396   0.012049    -0.114224   -1.195929\n2013-01-04  2.303441    0.536666    -1.013810   -0.574154\n2013-01-05  -1.327828   -0.003089   0.662432    0.038886\n\n#绘图\n\nimport matplotlib.pyplot as plt\n\nplt.plot([1,2,3])\n\nplt.ylabel(&apos;some numbers&apos;)\n\nplt.show()\n\n​</code></pre>","text":"Jupyter Notebookmath Last Checkpoint: a few seconds ago (autosaved) [Python 3] Python 3 Code:import numpy as npnp.arange(10)array([0, 1, 2, ","link":"","raw":null,"photos":[],"categories":[{"name":"Python","slug":"Python","count":41,"path":"api/categories/Python.json"},{"name":"机器学习","slug":"Python/机器学习","count":2,"path":"api/categories/Python/机器学习.json"},{"name":"相关库","slug":"Python/机器学习/相关库","count":1,"path":"api/categories/Python/机器学习/相关库.json"}],"tags":[{"name":"Python","slug":"Python","count":65,"path":"api/tags/Python.json"},{"name":"机器学习","slug":"机器学习","count":4,"path":"api/tags/机器学习.json"}]},{"title":"聊天室内核从0开始 – 3 Seq2Seq","slug":"seq2seq","date":"2019-01-06T14:32:09.000Z","updated":"2019-07-03T13:51:36.863Z","comments":true,"path":"api/articles/seq2seq.json","excerpt":"","keywords":null,"cover":null,"content":"<h1 id=\"Seq2Seq\"><a href=\"#Seq2Seq\" class=\"headerlink\" title=\"Seq2Seq\"></a>Seq2Seq</h1><p>关于Seq2Seq模型,看如下博客即可,针对的视频讲解可以去BliBli搜相关视频,基本一致. <a href=\"https://blog.csdn.net/wangyangzhizhou/article/details/77883152\" title=\"深度学习的seq2seq模型\" target=\"_blank\" rel=\"noopener\">深度学习的seq2seq模型</a></p>\n<blockquote>\n<p>注意要看下注意力机制</p>\n</blockquote>\n<h1 id=\"tensorflow-基础\"><a href=\"#tensorflow-基础\" class=\"headerlink\" title=\"tensorflow 基础\"></a>tensorflow 基础</h1><h1 id=\"项目完整代码-有空写其他的\"><a href=\"#项目完整代码-有空写其他的\" class=\"headerlink\" title=\"项目完整代码(有空写其他的)\"></a>项目完整代码(有空写其他的)</h1><blockquote>\n<p>这个项目中仅有一个训练一轮的模型(所以仅供看看).如果有需要还是要训练多点,但是前提当然是你有一个好的机器或者服务器,如果有信用卡也可以选择取Github Cloud 或者AWS上选择免费的服务器来帮你训练(时间会很长)</p>\n</blockquote>\n<p><a href=\"https://pan.baidu.com/s/1D18ZwROdqyilBuy6EU4xLA\" title=\"chatbot2\" target=\"_blank\" rel=\"noopener\">chatbot2</a></p>\n","text":"Seq2Seq关于Seq2Seq模型,看如下博客即可,针对的视频讲解可以去BliBli搜相关视频,基本一致. 深度学习的seq2seq模型注意要看下注意力机制tensorflow 基础项目完整代码(有空写其他的)这个项目中仅有一个训练一轮的模型(所以仅供看看).如果有需要还是要训","link":"","raw":null,"photos":[],"categories":[{"name":"Python","slug":"Python","count":41,"path":"api/categories/Python.json"},{"name":"机器学习","slug":"Python/机器学习","count":2,"path":"api/categories/Python/机器学习.json"},{"name":"聊天机器人内核","slug":"Python/机器学习/聊天机器人内核","count":1,"path":"api/categories/Python/机器学习/聊天机器人内核.json"}],"tags":[{"name":"Python","slug":"Python","count":65,"path":"api/tags/Python.json"},{"name":"机器学习","slug":"机器学习","count":4,"path":"api/tags/机器学习.json"},{"name":"聊天机器人内核","slug":"聊天机器人内核","count":3,"path":"api/tags/聊天机器人内核.json"}]},{"title":"机器学习实战(一) K-近邻","slug":"machine-learning-knn","date":"2018-01-27T14:04:15.000Z","updated":"2019-07-03T13:51:36.861Z","comments":true,"path":"api/articles/machine-learning-knn.json","excerpt":"","keywords":null,"cover":"http://be-sunshine.cn/wp-content/uploads/2018/01/QQ%E6%88%AA%E5%9B%BE20180127214341.jpg","content":"<h1 id=\"K临近算法概述\"><a href=\"#K临近算法概述\" class=\"headerlink\" title=\"K临近算法概述\"></a>K临近算法概述</h1><p>简单地说,k临近算法就是采用不同的特征值之间的距离方法进行分类. 通过数据与数据集间的距离进行分类,以及断定新数据的类别. 这里我们选择使用欧氏距离来当做两点间的距离.</p>\n<h2 id=\"实现KNN算法\"><a href=\"#实现KNN算法\" class=\"headerlink\" title=\"实现KNN算法\"></a>实现KNN算法</h2><h3 id=\"伪码\"><a href=\"#伪码\" class=\"headerlink\" title=\"伪码\"></a>伪码</h3><blockquote>\n<p>对未知类别属性的数据集中的每个点依次执行以下操作</p>\n<blockquote>\n<p>计算已知类别数据集中的点 按照距离递增次序排序 选取与当前点距离最小的k个点 确定前k个点所在的类别的出现频率 返回前k个点出现频率最高的类别作为当前点的预测分类</p>\n</blockquote>\n</blockquote>\n<h3 id=\"实现算法前\"><a href=\"#实现算法前\" class=\"headerlink\" title=\"实现算法前\"></a>实现算法前</h3><p>我们来学习一下需要用到的一些库函数.</p>\n<h4 id=\"numpy\"><a href=\"#numpy\" class=\"headerlink\" title=\"numpy\"></a>numpy</h4><p>1.list转array</p>\n<pre><code>from numpy import *\narray([1,1])</code></pre><p>2.zeros()初始化向量</p>\n<pre><code>import numpy\nfrom numpy import *\na=(3,4)\nzeros(a)\n# 初始化一个3行四列的0矩阵</code></pre><p>3.矩阵操作</p>\n<pre><code>import numpy\nfrom numpy import *\n\nMat = array([[1,2],[3,4]])\n\n# 每行最小\nMat.min(0)\n# 每列最小\nMat.min(1)\n# 每行和\nMat.sum(0)\n# 上面传递的参数都是axis=1 or 0,0代表行,1代表列\n\n# shape返回一个tuple,代表矩阵的行数和列数\nMat.shape</code></pre><p>3.1矩阵排序argsort()</p>\n<pre><code>import numpy\nfrom numpy import *\n\nk = array([1,2,8.5,-1,0])\nt = k.argsort()\n# 输出升序排序后每位数字的下标数组</code></pre><p>输出升序排序后每位数字的下标数组,比如上面那个输出是:</p>\n<pre><code>array([3,4,0,1,2],dtype=int64)\n# 第一个是k[3],第二个是k[4]</code></pre><p>4.tile</p>\n<pre><code>import numpy\nfrom numpy import *\n\n# 有两个参数,第一个参数是初始矩阵,第二个参数是一个tuple,代表\n# 向行拓展次数,以及向列拓展次数,具体调用一下就知道了\ntile([1,2],(1))# 原矩阵\ntile([1,2],(2,2))# 行两倍,列两倍</code></pre><p>5.运算 直接使用运算符号,是相当于每行与每列进行运算. 真正的矩阵运算需要通过库来实现.</p>\n<h4 id=\"数据读取\"><a href=\"#数据读取\" class=\"headerlink\" title=\"数据读取\"></a>数据读取</h4><p>与本例相关的数据集地址: <a href=\"http://be-sunshine.cn:9011/static/file/datingTestSet2.txt\" title=\"datingTestSet2.txt\" target=\"_blank\" rel=\"noopener\">datingTestSet2.txt</a></p>\n<pre><code># 打开数据文件\nfr = open(&apos;datingTestSet2.txt&apos;)\n# 按行读取\narrayOfLines = fr.readlines()\narrayOfLines\n\nfrom numpy import *\nnumberOfLines = len(arrayOfLines)\n# 生成与数据集相同列数的矩阵\nreturnMat = zeros((numberOfLines,3))\nreturnMat\n# 格式化读入,存储到矩阵中\nfor line in arrayOfLines:\n    line = line.strip()\n    print(line.split(&apos;\\t&apos;))\n    print(int(line.split(&apos;\\t&apos;)[-1]))</code></pre><h4 id=\"matplotlib散点图\"><a href=\"#matplotlib散点图\" class=\"headerlink\" title=\"matplotlib散点图\"></a>matplotlib散点图</h4><pre><code>import matplotlib\nimport matplotlib.pyplot as plt\nimport numpy\nfrom numpy import *\n# 生成plt\nfig = plt.figure()\n# 规定最多111个点\nax = fig.add_subplot(111)\n# 创建一个矩阵,第三个代表类别\nMat = array([[1,123,2],[10,256,1],[7,321,3]])\n# 获取类别矩阵\nLabel = Mat[:,2]\n# 第一个参数横坐标,第二个参数纵坐标,第三个参数,颜色矩阵,第三个参数,大小矩阵\nax.scatter(Mat[:,0],Mat[:,1],15.0*Label,15.0*Label)\n# 绘制\nplt.show()</code></pre><h3 id=\"代码实现\"><a href=\"#代码实现\" class=\"headerlink\" title=\"代码实现\"></a>代码实现</h3><h4 id=\"对于代码的解释我都注释在代码中了\"><a href=\"#对于代码的解释我都注释在代码中了\" class=\"headerlink\" title=\"对于代码的解释我都注释在代码中了\"></a>对于代码的解释我都注释在代码中了</h4><pre><code># K-近邻\n&apos;&apos;&apos;\n算法思想:\n\n计算已知类别数据集中的点\n\n按照距离递增次序排序\n\n选取与当前点距离最小的k个点\n\n确定前k个点所在的类别的出现频率\n\n返回前k个点出现频率最高的类别作为当前点的预测分类\n&apos;&apos;&apos;\n\ndef classify0(inX,dataSet,labels,k):\n    &apos;&apos;&apos;\n    k-邻近算法\n    inX:测试数据 - array\n    dataSet:样本数据集 - array\n    labels:标签向量 - array\n    k: 选举前k个 - int\n    &apos;&apos;&apos;\n    # 获取数据集的列数\n    dataSetSize = dataSet.shape[0]\n    # 新建一个矩阵,将测试数据inX复制到每列上,以便计算距离\n    diffMat = tile(inX,(dataSetSize,1)) - dataSet\n    # 对每个指标的距离进行平方\n    sqDiffMat = diffMat**2\n    # 把每个指标的差方相加\n    sqDistance = sqDiffMat.sum(0)\n    # 计算inX与每个点的距离\n    distance = sqDistance**0.5\n    # 升序排序,返回排序后的下标矩阵\n    sortedDistIndicies = distance.argsort()\n\n    # 选择距离最小的k个点\n    classCount = {}\n    for i in range(k):\n        # 选取前k个距离最近的点中的第i个\n        voteIlabel = labels[sortedDistIndicies[i]]\n        # 映射到dict中,其中get的第二个参数是如果不存在的默认值\n        classCount[voteIlabel] = classCount.get(voteIlabel,0) + 1\n    # dict.items()返回一个字典列表(dict_items)类型,即dict的原始插入顺序的list\n    # 可以直接用sorted排序\n    # 升序,其中operator.itemgetter(index)代表按照待排列表的第几个元素排序.\n    # reverse=True即变成了降序\n    sortedClassCount = sorted(classCount.items(),key=operator.itemgetter(1),reverse=True)\n    # 返回分类中频率最高的那个的标签\n    return sortedClassCount[0][0]</code></pre><h3 id=\"可视化分析\"><a href=\"#可视化分析\" class=\"headerlink\" title=\"可视化分析\"></a>可视化分析</h3><pre><code>import numpy\nfrom numpy import *\n# 将测试数据转换为需要的类型\ndef file2matrix(filename):\n    &apos;&apos;&apos;\n    对于datingTestSet2.txt返回值类型\n    returnMat: [里程数,百分比,公升数]\n    --每年获得的飞行常客里程数\n    --玩视频游戏所耗时间百分比\n    --每周消耗的冰淇淋公升数\n    classLabelVector: [标签]\n    --1,2,3分别代表最好,其次,最次\n    &apos;&apos;&apos;\n    fr = open(filename)\n    arrayOLines = fr.readlines()\n    # 得到文件行数\n    numberOfLines = len(arrayOLines)\n    # 新建(文件行数,3列)的0 array\n    returnMat = zeros((numberOfLines,3))\n    classLabelVector = []\n    index = 0\n    # 处理数据\n    for line in arrayOLines:\n        line = line.strip()\n        listFromLine = line.split(&apos;\\t&apos;)\n        # 将数据加入返回的列表中\n        returnMat[index,:] = listFromLine[:3]\n        # 标签列表\n        classLabelVector.append(int(listFromLine[-1]))\n        index+=1\n    return returnMat,classLabelVector\n\nimport matplotlib\nimport matplotlib.pyplot as plt\n\ndatingDataMat,datingLabels = file2matrix(&apos;datingTestSet2.txt&apos;)\n\nfig = plt.figure()\nax = fig.add_subplot(111)\nax.scatter(datingDataMat[:,0],datingDataMat[:,1],15.0*array(datingLabels),15.0*array(datingLabels))\nplt.show()</code></pre><p>最后的结果如下: <a href=\"http://be-sunshine.cn/wp-content/uploads/2018/01/QQ截图20180127214341.jpg\" target=\"_blank\" rel=\"noopener\"><img src=\"http://be-sunshine.cn/wp-content/uploads/2018/01/QQ%E6%88%AA%E5%9B%BE20180127214341.jpg\" alt></a></p>\n<h3 id=\"归一化数值\"><a href=\"#归一化数值\" class=\"headerlink\" title=\"归一化数值\"></a>归一化数值</h3><p>我们可以发现,在数据集中,每种类的数据极差差距都很大,比如飞行常客里程数的极差,和每周消费冰淇淋公升数的极差相距交大. 所以我们尝试将不同的数据集按照相同的区间范围进行计算. 计算公式(和百分制化为150分制的道理一样): <strong>newValue = (OldValue-min)/(max-min)</strong> 其中min和max代表数据集中的最小特征值和最大特征值. 是用这个公式后的数值将统一变成0<del>1或者-1</del>1之间.</p>\n<h4 id=\"代码\"><a href=\"#代码\" class=\"headerlink\" title=\"代码\"></a>代码</h4><pre><code>def autoNorm(dataSet):\n    &apos;&apos;&apos;\n    归一化数值\n    返回值\n    normDataSet:归一化后数值 - array\n    ranges:每类特征极差 - array\n    minVals:每类特征最小值 - array\n    &apos;&apos;&apos;\n    # numpy数组 .min(0)每列最小值\n    # .min(1)每行最小值\n    minVals = dataSet.min(0)\n    maxVals = dataSet.max(0)\n    ranges = maxVals - minVals\n    normDataSet = zeros(shape(dataSet))\n    m = dataSet.shape[0]\n    normDataSet = dataSet - tile(minVals,(m,1))\n    normDataSet = normDataSet/tile(ranges,(m,1))\n    return normDataSet,ranges,minVals</code></pre><h3 id=\"对约会网站的测试\"><a href=\"#对约会网站的测试\" class=\"headerlink\" title=\"对约会网站的测试\"></a>对约会网站的测试</h3><p>最后我们对之前的datingTestSet2.txt进行误差测试 其中hoRatio代表对数据集的测试普及率. 这里用<strong>0.1即1000*0.1=100个</strong>样本数据进行测试.</p>\n<pre><code>def datingClassTest():\n    hoRatio = 0.10\n    datingDataMat,datingLabels = file2matrix(&apos;datingTestSet2.txt&apos;)\n    normMat,ranges,minVals = autoNorm(datingDataMat)\n    m = normMat.shape[0]\n    numTestVecs = int(m*hoRatio)\n    errorCount = 0.0\n    for i in range(numTestVecs):\n        classifierResult = classify0(normMat[i,:],normMat[numTestVecs:m,:],\\\n                                    datingLabels[numTestVecs:m],3)\n        print(&apos;the classifier came back with: %d,the real answer is: %d&apos;\\\n              % (classifierResult,datingLabels[i]))\n        if(classifierResult != datingLabels[i]): errorCount += 1.0\n    print(&apos;the total error rate is: %f&apos; % (errorCount/float(numTestVecs)))\n\ndatingClassTest()</code></pre><p>测试结果:</p>\n<pre><code>the classifier came back with: 1,the real answer is: 3\nthe classifier came back with: 1,the real answer is: 2\nthe classifier came back with: 3,the real answer is: 1\nthe classifier came back with: 1,the real answer is: 1\nthe classifier came back with: 3,the real answer is: 1\nthe classifier came back with: 1,the real answer is: 1\nthe classifier came back with: 1,the real answer is: 3\nthe classifier came back with: 3,the real answer is: 3\nthe classifier came back with: 1,the real answer is: 1\nthe classifier came back with: 3,the real answer is: 3\nthe classifier came back with: 1,the real answer is: 1\nthe classifier came back with: 1,the real answer is: 1\nthe classifier came back with: 1,the real answer is: 2\nthe classifier came back with: 1,the real answer is: 1\nthe classifier came back with: 2,the real answer is: 1\nthe classifier came back with: 3,the real answer is: 1\nthe classifier came back with: 1,the real answer is: 1\nthe classifier came back with: 1,the real answer is: 1\nthe classifier came back with: 3,the real answer is: 2\nthe classifier came back with: 3,the real answer is: 3\nthe classifier came back with: 1,the real answer is: 2\nthe classifier came back with: 3,the real answer is: 1\nthe classifier came back with: 1,the real answer is: 2\nthe classifier came back with: 1,the real answer is: 3\nthe classifier came back with: 1,the real answer is: 2\nthe classifier came back with: 3,the real answer is: 3\nthe classifier came back with: 1,the real answer is: 2\nthe classifier came back with: 3,the real answer is: 3\nthe classifier came back with: 2,the real answer is: 2\nthe classifier came back with: 3,the real answer is: 1\nthe classifier came back with: 3,the real answer is: 3\nthe classifier came back with: 3,the real answer is: 1\nthe classifier came back with: 1,the real answer is: 3\nthe classifier came back with: 1,the real answer is: 1\nthe classifier came back with: 1,the real answer is: 2\nthe classifier came back with: 1,the real answer is: 1\nthe classifier came back with: 1,the real answer is: 1\nthe classifier came back with: 1,the real answer is: 2\nthe classifier came back with: 3,the real answer is: 3\nthe classifier came back with: 3,the real answer is: 3\nthe classifier came back with: 1,the real answer is: 1\nthe classifier came back with: 3,the real answer is: 2\nthe classifier came back with: 1,the real answer is: 3\nthe classifier came back with: 3,the real answer is: 3\nthe classifier came back with: 3,the real answer is: 3\nthe classifier came back with: 3,the real answer is: 1\nthe classifier came back with: 3,the real answer is: 1\nthe classifier came back with: 3,the real answer is: 1\nthe classifier came back with: 1,the real answer is: 1\nthe classifier came back with: 1,the real answer is: 2\nthe classifier came back with: 1,the real answer is: 2\nthe classifier came back with: 2,the real answer is: 1\nthe classifier came back with: 3,the real answer is: 3\nthe classifier came back with: 1,the real answer is: 2\nthe classifier came back with: 3,the real answer is: 2\nthe classifier came back with: 1,the real answer is: 2\nthe classifier came back with: 1,the real answer is: 2\nthe classifier came back with: 1,the real answer is: 3\nthe classifier came back with: 1,the real answer is: 1\nthe classifier came back with: 1,the real answer is: 2\nthe classifier came back with: 1,the real answer is: 1\nthe classifier came back with: 1,the real answer is: 2\nthe classifier came back with: 1,the real answer is: 2\nthe classifier came back with: 1,the real answer is: 2\nthe classifier came back with: 1,the real answer is: 2\nthe classifier came back with: 1,the real answer is: 2\nthe classifier came back with: 2,the real answer is: 3\nthe classifier came back with: 1,the real answer is: 2\nthe classifier came back with: 3,the real answer is: 3\nthe classifier came back with: 1,the real answer is: 1\nthe classifier came back with: 1,the real answer is: 2\nthe classifier came back with: 3,the real answer is: 3\nthe classifier came back with: 3,the real answer is: 2\nthe classifier came back with: 1,the real answer is: 2\nthe classifier came back with: 1,the real answer is: 1\nthe classifier came back with: 1,the real answer is: 3\nthe classifier came back with: 2,the real answer is: 1\nthe classifier came back with: 1,the real answer is: 1\nthe classifier came back with: 1,the real answer is: 3\nthe classifier came back with: 1,the real answer is: 3\nthe classifier came back with: 1,the real answer is: 1\nthe classifier came back with: 1,the real answer is: 2\nthe classifier came back with: 3,the real answer is: 3\nthe classifier came back with: 1,the real answer is: 1\nthe classifier came back with: 3,the real answer is: 3\nthe classifier came back with: 1,the real answer is: 1\nthe classifier came back with: 1,the real answer is: 2\nthe classifier came back with: 1,the real answer is: 2\nthe classifier came back with: 2,the real answer is: 1\nthe classifier came back with: 1,the real answer is: 1\nthe classifier came back with: 1,the real answer is: 3\nthe classifier came back with: 1,the real answer is: 3\nthe classifier came back with: 2,the real answer is: 1\nthe classifier came back with: 1,the real answer is: 2\nthe classifier came back with: 1,the real answer is: 1\nthe classifier came back with: 1,the real answer is: 3\nthe classifier came back with: 1,the real answer is: 3\nthe classifier came back with: 1,the real answer is: 2\nthe classifier came back with: 3,the real answer is: 1\nthe classifier came back with: 1,the real answer is: 1\nthe total error rate is: 0.600000</code></pre><h1 id=\"Done-And-thank-you-for-watching\"><a href=\"#Done-And-thank-you-for-watching\" class=\"headerlink\" title=\"Done,And thank you for watching!\"></a>Done,And thank you for watching!</h1>","text":"K临近算法概述简单地说,k临近算法就是采用不同的特征值之间的距离方法进行分类. 通过数据与数据集间的距离进行分类,以及断定新数据的类别. 这里我们选择使用欧氏距离来当做两点间的距离.实现KNN算法伪码对未知类别属性的数据集中的每个点依次执行以下操作计算已知类别数据集中的点 按照距","link":"","raw":null,"photos":[],"categories":[{"name":"K-邻近","slug":"K-邻近","count":1,"path":"api/categories/K-邻近.json"},{"name":"Python","slug":"K-邻近/Python","count":1,"path":"api/categories/K-邻近/Python.json"},{"name":"机器学习","slug":"K-邻近/Python/机器学习","count":1,"path":"api/categories/K-邻近/Python/机器学习.json"}],"tags":[{"name":"Python","slug":"Python","count":65,"path":"api/tags/Python.json"},{"name":"机器学习","slug":"机器学习","count":4,"path":"api/tags/机器学习.json"},{"name":"K-邻近","slug":"K-邻近","count":1,"path":"api/tags/K-邻近.json"}]},{"title":"聊天室内核从0开始 - 1 前置知识与NLP","slug":"type-2","date":"2019-01-01T13:02:42.000Z","updated":"2019-07-03T13:51:36.863Z","comments":true,"path":"api/articles/type-2.json","excerpt":"","keywords":null,"cover":"http://be-sunshine.cn/wp-content/uploads/2019/01/1234352-13d969531284a9f9.png","content":"<blockquote>\n<p>最后更新于2019/1/6</p>\n</blockquote>\n<h1 id=\"前置知识\"><a href=\"#前置知识\" class=\"headerlink\" title=\"前置知识\"></a>前置知识</h1><h2 id=\"TensorFlow\"><a href=\"#TensorFlow\" class=\"headerlink\" title=\"TensorFlow\"></a>TensorFlow</h2><blockquote>\n<ol>\n<li>张量(Tensor)</li>\n<li>图(Flow-&gt;Graph)</li>\n<li>会话(Session)</li>\n</ol>\n</blockquote>\n<h3 id=\"张量-Tensor\"><a href=\"#张量-Tensor\" class=\"headerlink\" title=\"张量(Tensor)\"></a>张量(Tensor)</h3><blockquote>\n<p>类似于矩阵,一维的张量叫做向量</p>\n</blockquote>\n<h3 id=\"计算图-Graph\"><a href=\"#计算图-Graph\" class=\"headerlink\" title=\"计算图(Graph)\"></a>计算图(Graph)</h3><blockquote>\n<p>TensorFlow的计算图的组成和数据结构中的图不同.</p>\n<blockquote>\n<ol>\n<li>图的节点: op-&gt;即操作.</li>\n<li>图的边: 即数据流,此处的数据流就是上述张量.</li>\n</ol>\n</blockquote>\n</blockquote>\n<h3 id=\"会话-Session\"><a href=\"#会话-Session\" class=\"headerlink\" title=\"会话(Session)\"></a>会话(Session)</h3><blockquote>\n<p>在TensorFlow中,要想启动一个图的前提是要先创建一个会话,Ts所有对图的操作,都必须在会话中进行.</p>\n</blockquote>\n<h3 id=\"模型训练一般流程\"><a href=\"#模型训练一般流程\" class=\"headerlink\" title=\"模型训练一般流程\"></a>模型训练一般流程</h3><p>document.write(“graph TD\\nA(开始) –&gt; B(定义数据集)\\nB –&gt; C(定义模型)\\nC –&gt;D(编写并训练模型)\\nD –&gt;E(模型测试)\\n”);</p>\n<h2 id=\"Android\"><a href=\"#Android\" class=\"headerlink\" title=\"Android\"></a>Android</h2><h3 id=\"模拟机替代AVG\"><a href=\"#模拟机替代AVG\" class=\"headerlink\" title=\"模拟机替代AVG\"></a>模拟机替代AVG</h3><p>为了解决Android Studio使用AVG虚拟机时会导致内存占用过大的问题. 这里我们选用网易旗下的MuMu模拟器.</p>\n<h4 id=\"步骤\"><a href=\"#步骤\" class=\"headerlink\" title=\"步骤\"></a>步骤</h4><p>document.write(“graph TD\\nA(开启MuMu模拟器) –&gt; B(进入到MuMu根目录的上层目录)\\nB –&gt; C(找到vmonitorbin)\\nC –&gt;D(在cmd中进入到上述地址-先进入盘符)\\nD –&gt;E(输入adb_server.exe connect port-如7555)\\n”);</p>\n<p>注意:这个必须在Android Studio开启时连接才可以,如果出现Empty host name,多连接几次就可以了. 使用这个方法就可以有效减少内存的占用问题.</p>\n<h2 id=\"神经网络\"><a href=\"#神经网络\" class=\"headerlink\" title=\"神经网络\"></a>神经网络</h2><h3 id=\"神经网络相关学习视频\"><a href=\"#神经网络相关学习视频\" class=\"headerlink\" title=\"神经网络相关学习视频\"></a>神经网络相关学习视频</h3><p><a href=\"https://www.icourse163.org/learn/DA-1002183004?tid=1002301011#/learn/content\" target=\"_blank\" rel=\"noopener\">https://www.icourse163.org/learn/DA-1002183004?tid=1002301011#/learn/content</a> 因为其中涉及内容过多,且基础,故我在此仅讲解几个比较生疏难懂的概念,而不做展开,其基础推荐在学号高数和概率论以及线性代数这三门课程以后再进行展开(也可以不需要,但理解上会有些困难).</p>\n<h3 id=\"梯度下降\"><a href=\"#梯度下降\" class=\"headerlink\" title=\"梯度下降\"></a>梯度下降</h3><blockquote>\n<p>在了解梯度下降前你需要知道</p>\n<blockquote>\n<ol>\n<li>计算图的节点是简单的操作</li>\n<li>高数求导中的链式求导法</li>\n<li>既然它们的节点时简单的运算,那么就可以很方便地使用链式求导法则对其进行求导 &gt; 比如 Y=Z+b,Z=b+2 dY/db=dZ/db+d(b)/db</li>\n</ol>\n</blockquote>\n<p>则如下图所示 <img src=\"http://be-sunshine.cn/wp-content/uploads/2019/01/1234352-13d969531284a9f9.png\" alt> 计算图是正向传播的,而计算图中的反向计算梯度是反向计算的,即函数的上升方向. 故反向传播以后计算出来的梯度只需要取负即是梯度下降的方向了.</p>\n</blockquote>\n<p>以上基础知识熟悉以后可以看这篇文章: <a href=\"https://www.jianshu.com/p/c7e642877b0e\" title=\"深入浅出,梯度下降法及其实现\" target=\"_blank\" rel=\"noopener\">深入浅出,梯度下降法及其实现</a></p>\n<h3 id=\"向量化\"><a href=\"#向量化\" class=\"headerlink\" title=\"向量化\"></a>向量化</h3><blockquote>\n<p>这个不难理解,即计算图中数据流都是向量,大大缩短计算时间.</p>\n</blockquote>\n<h3 id=\"Python广播机制\"><a href=\"#Python广播机制\" class=\"headerlink\" title=\"Python广播机制\"></a>Python广播机制</h3><blockquote>\n<p>举几个例子吧,具体在实践中总结,或查阅相关DOC.</p>\n</blockquote>\n<pre><code>A = numpy.array([1,2,3])\nresult = A + 100\nprint(result)\n\n输出: [101 102 103]</code></pre><h3 id=\"激活函数\"><a href=\"#激活函数\" class=\"headerlink\" title=\"激活函数\"></a>激活函数</h3><blockquote>\n<p>计算图的每层节点将上一层的输出作为本层的输入,如果没有激活函数,那么最终的结果等效于F(x)=x即线性函数. 而激活函数则是在层与层间添加上一个激活函数,使之并不完全作为线性函数来传递数据(即进入什么数据,出来就一定是唯一结果) 即由激活函数判断是否输出</p>\n</blockquote>\n<h3 id=\"神经元模型-参考书籍-西瓜书\"><a href=\"#神经元模型-参考书籍-西瓜书\" class=\"headerlink\" title=\"神经元模型(参考书籍-西瓜书)\"></a>神经元模型(参考书籍-西瓜书)</h3><p><img src=\"http://be-sunshine.cn/wp-content/uploads/2019/01/20180507224752660.png\" alt></p>\n<blockquote>\n<p>在生物神经网络中,，每个神经元与其他神经元相连,当它”兴奋”时,就会向相连的神经元发送化学物质,从而改变这些神经元内的电位,如果某神经元的电位超过一个”阈值”,那么他就会被激活,即”兴奋”起来,向其他神经元发送化学物质. 现在最常用的组成神经网络的节点神经元模型是M-P神经元模型.在这个模型中,神经元接收到来自其它n个神经元传递过来的输入信号,这些输入信号通过带权重的连接进行传递,神经元接收到的总输入值将与神经元的阈值进行比较,然后通过”激活函数”处理以产生神经元的输出.</p>\n</blockquote>\n<h3 id=\"神经网络分类\"><a href=\"#神经网络分类\" class=\"headerlink\" title=\"神经网络分类\"></a>神经网络分类</h3><blockquote>\n<p>大部分在上述的视频中都有系统介绍,这里我只做总结</p>\n</blockquote>\n<h4 id=\"感知机与深层神经网络\"><a href=\"#感知机与深层神经网络\" class=\"headerlink\" title=\"感知机与深层神经网络\"></a>感知机与深层神经网络</h4><p><img src=\"http://be-sunshine.cn/wp-content/uploads/2019/01/p.png\" alt></p>\n<blockquote>\n<p>即 输入层 -&gt; 隐藏层 -&gt; 输出层</p>\n</blockquote>\n<h4 id=\"BP神经网络\"><a href=\"#BP神经网络\" class=\"headerlink\" title=\"BP神经网络\"></a>BP神经网络</h4><blockquote>\n<p>BP网络即前向传播+反向传播来更新偏置. 特点:</p>\n<blockquote>\n<p>1-可以通过逐层信息传递到最后的输出. 2-沿着一条直线计算,直到最后一层,求出计算结果. 3-包含输入层、输出层和隐藏层,其目的是实现从输入到输出的映射. 4-一般包含多层,并且层与层之间是全连接的,不存在同层和跨层连接.</p>\n</blockquote>\n</blockquote>\n<h4 id=\"循环神经网络RNN和LSTM\"><a href=\"#循环神经网络RNN和LSTM\" class=\"headerlink\" title=\"循环神经网络RNN和LSTM\"></a>循环神经网络RNN和LSTM</h4><blockquote>\n<p>这类计算图是针对于成序列的数据的.</p>\n<blockquote>\n<p>类似于造句,造音乐等.如果一个序列过长,则会导致可能在计算后面序列的时候将前面序列的影响变低.从而导致序列无法有效处理”长期依赖”的问题.</p>\n</blockquote>\n</blockquote>\n<h5 id=\"RNN\"><a href=\"#RNN\" class=\"headerlink\" title=\"RNN\"></a>RNN</h5><h6 id=\"前向传播\"><a href=\"#前向传播\" class=\"headerlink\" title=\"前向传播\"></a>前向传播</h6><blockquote>\n<p>普通的CNN模型即上述的神经网络模型,而NLP遵守的规则一般为对于一个句子的分析,每个单词的分析,如果采取上述的线性模型可能会导致语言没有一点逻辑,即 “我/爱/你” - 则针对我输出A,针对爱输出B,针对你输出C 得到结果ABC. 而RNN最大的特点在于其可记忆性.什么叫可记忆性呢?即上一个单词的结果要传递给下一层,使下一个单词运算出的结果可以结合上一个单词以及当前的单词一起得出最后的结果.</p>\n</blockquote>\n<p><img src=\"http://be-sunshine.cn/wp-content/uploads/2019/01/1042406-20170306142253375-175971779.png\" alt></p>\n<blockquote>\n<p>如上图,右侧是拆分后的RNN,x(i)代表的是第i个单词,而计算后的h(i)会传递给h(i+1),y(i)是由上一层计算出来的词向量. 数学公式表达为:</p>\n<blockquote>\n<p>h(i)=g(w*h(i-1)+w*x(i)+bh) g(激活函数)一般选为tanh/Relu y(i)=g(w*h(i)+by) g一般选择sigmod/softmax</p>\n</blockquote>\n<p>优化: 因为词向量的行是相同的,所以将列拼在一起即可.</p>\n<blockquote>\n<p>h(i)=g(w*[h(i-1),x(i)]+bh)</p>\n</blockquote>\n</blockquote>\n<h6 id=\"反向传播\"><a href=\"#反向传播\" class=\"headerlink\" title=\"反向传播\"></a>反向传播</h6><blockquote>\n<p>反向传播作用依然是：减少误差,计算lost函数. 用倒数来计算某一个节点队最终结果的影响程度.训练完后,取平均值(大概,这点我没太仔细看).</p>\n</blockquote>\n<p>一个比较通俗易懂的链接: <a href=\"https://blog.csdn.net/shaomingliang499/article/details/50587300\" title=\" 一步一步教你反向传播\" target=\"_blank\" rel=\"noopener\">一步一步教你反向传播</a></p>\n<h6 id=\"RNN的几种类型\"><a href=\"#RNN的几种类型\" class=\"headerlink\" title=\"RNN的几种类型\"></a>RNN的几种类型</h6><p><img src=\"http://be-sunshine.cn/wp-content/uploads/2019/01/20180719232051968.jpg\" alt></p>\n<h5 id=\"LSTM\"><a href=\"#LSTM\" class=\"headerlink\" title=\"LSTM\"></a>LSTM</h5><blockquote>\n<p>再清楚了RNN以后,LSTM其实就是基于RNN的一个变种.</p>\n<blockquote>\n<p>因为RNN实际应用中无法解决长效记忆的问题,所以催生出了LSTM这一模型.放一个简单的视频可以看下.</p>\n</blockquote>\n<p><a href=\"https://www.bilibili.com/video/av15998549?from=search&seid=17651800282007333668\" title=\"什么是 LSTM RNN 循环神经网络 ?\" target=\"_blank\" rel=\"noopener\">什么是 LSTM RNN 循环神经网络 ?</a></p>\n</blockquote>\n<h2 id=\"NLP\"><a href=\"#NLP\" class=\"headerlink\" title=\"NLP\"></a>NLP</h2><h3 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h3><blockquote>\n<p>自然语言处理,探索如何处理及运用自然语言,即让电脑懂人类的语言. 包含文本分析、信息检索、词性标注、问答系统等.</p>\n</blockquote>\n<ol>\n<li><p>词法分析</p>\n<blockquote>\n<p>分词技术、词性标注(名词n,形容词a,副词d,人称代词rr,动词v…)、命名实体识别、词义消歧</p>\n</blockquote>\n</li>\n<li><p>句法分析</p>\n</li>\n<li><p>语义分析</p>\n</li>\n</ol>\n<h3 id=\"分词技术\"><a href=\"#分词技术\" class=\"headerlink\" title=\"分词技术\"></a>分词技术</h3><blockquote>\n<p>中科院分词系统(nlpir): <a href=\"http://ictclas.nlpir.org/nlpir/\" title=\"语义分词系统\" target=\"_blank\" rel=\"noopener\">中科院语义分词系统</a></p>\n</blockquote>\n<h3 id=\"命名实体识别\"><a href=\"#命名实体识别\" class=\"headerlink\" title=\"命名实体识别\"></a>命名实体识别</h3><blockquote>\n<p>即分词方法. 命名实体识别（Named Entity Recognition，简称NER），又称作“专名识别”，是指识别文本中具有特定意义的实体，主要包括人名、地名、机构名、专有名词等。 一般分为两种方法</p>\n<blockquote>\n<p>基于规则和词典的方法.</p>\n</blockquote>\n</blockquote>\n<p>document.write(“graph TD\\n\\nC(基于统计的方法)\\nC –&gt;D[隐马尔可夫模型]\\nC –&gt;E[较大熵]\\nC –&gt;F[支持向量机]\\nC –&gt;G[条件随机场]\\n”);</p>\n<h3 id=\"朴素贝叶斯\"><a href=\"#朴素贝叶斯\" class=\"headerlink\" title=\"朴素贝叶斯\"></a>朴素贝叶斯</h3><p>即概率论中的贝叶斯概型.</p>\n<blockquote>\n<p>在为序列定型中的用法:</p>\n<blockquote>\n<p>如: 分次以后判断每个单词是垃圾邮件的可能性大小,再用朴素贝叶斯计算出该邮件是垃圾邮件的概率.</p>\n</blockquote>\n</blockquote>\n<h3 id=\"马尔科夫过程\"><a href=\"#马尔科夫过程\" class=\"headerlink\" title=\"马尔科夫过程\"></a>马尔科夫过程</h3><blockquote>\n<p>其实我也没搞懂意义在哪~ (1)独立随机过程为马尔可夫过程。 (2)独立增量过程为马尔可夫过程：没{X(t)，t∈[0，+∞)}为一独立增量过程，且有P(X(0)=x0)=1，x0为常数，则X(t)为马尔可夫过程。 (3)泊松过程为马尔可夫过程。 (4)维纳过程为马尔可夫过程。 (5)质点随机游动过程为马尔可夫过程。 即下一时刻的状态只依赖于上一时刻,而与上一时刻以前无关.</p>\n</blockquote>\n<h3 id=\"语料的处理方法\"><a href=\"#语料的处理方法\" class=\"headerlink\" title=\"语料的处理方法\"></a>语料的处理方法</h3><ol>\n<li>数据清洗(去掉无意义的标签,url,符号等)</li>\n<li>分词、大小写转换、添加句首句尾、词性标注.</li>\n<li>统计词频、抽取文本特征、特征选择、计算特征权重、归一化</li>\n<li>划分训练集、测试集（先分几份,然后7-3划分）</li>\n</ol>\n<h1 id=\"聊天室内核从0开始-–-2-处理语料库\"><a href=\"#聊天室内核从0开始-–-2-处理语料库\" class=\"headerlink\" title=\"聊天室内核从0开始 – 2 处理语料库\"></a>聊天室内核从0开始 – 2 处理语料库</h1><p><a href=\"http://be-sunshine.cn/index.php/2019/01/04/type-3/\" target=\"_blank\" rel=\"noopener\">http://be-sunshine.cn/index.php/2019/01/04/type-3/</a></p>\n","text":"最后更新于2019/1/6前置知识TensorFlow张量(Tensor)图(Flow-&gt;Graph)会话(Session)张量(Tensor)类似于矩阵,一维的张量叫做向量计算图(Graph)TensorFlow的计算图的组成和数据结构中的图不同.图的节点: op-&gt","link":"","raw":null,"photos":[],"categories":[{"name":"Android","slug":"Android","count":5,"path":"api/categories/Android.json"},{"name":"NLP","slug":"Android/NLP","count":1,"path":"api/categories/Android/NLP.json"},{"name":"TensorFlow","slug":"Android/NLP/TensorFlow","count":1,"path":"api/categories/Android/NLP/TensorFlow.json"},{"name":"机器学习","slug":"Android/NLP/TensorFlow/机器学习","count":1,"path":"api/categories/Android/NLP/TensorFlow/机器学习.json"},{"name":"聊天机器人内核","slug":"Android/NLP/TensorFlow/机器学习/聊天机器人内核","count":1,"path":"api/categories/Android/NLP/TensorFlow/机器学习/聊天机器人内核.json"}],"tags":[{"name":"Android","slug":"Android","count":5,"path":"api/tags/Android.json"},{"name":"机器学习","slug":"机器学习","count":4,"path":"api/tags/机器学习.json"},{"name":"聊天机器人内核","slug":"聊天机器人内核","count":3,"path":"api/tags/聊天机器人内核.json"},{"name":"NLP","slug":"NLP","count":1,"path":"api/tags/NLP.json"},{"name":"TensorFlow","slug":"TensorFlow","count":1,"path":"api/tags/TensorFlow.json"}]}]}