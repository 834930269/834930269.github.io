{"name":"聊天机器人内核","slug":"聊天机器人内核","count":3,"postlist":[{"title":"聊天室内核从0开始 – 3 Seq2Seq","slug":"seq2seq","date":"2019-01-06T14:32:09.000Z","updated":"2019-07-03T13:51:36.863Z","comments":true,"path":"api/articles/seq2seq.json","excerpt":"","keywords":null,"cover":null,"content":"<h1 id=\"Seq2Seq\"><a href=\"#Seq2Seq\" class=\"headerlink\" title=\"Seq2Seq\"></a>Seq2Seq</h1><p>关于Seq2Seq模型,看如下博客即可,针对的视频讲解可以去BliBli搜相关视频,基本一致. <a href=\"https://blog.csdn.net/wangyangzhizhou/article/details/77883152\" title=\"深度学习的seq2seq模型\" target=\"_blank\" rel=\"noopener\">深度学习的seq2seq模型</a></p>\n<blockquote>\n<p>注意要看下注意力机制</p>\n</blockquote>\n<h1 id=\"tensorflow-基础\"><a href=\"#tensorflow-基础\" class=\"headerlink\" title=\"tensorflow 基础\"></a>tensorflow 基础</h1><h1 id=\"项目完整代码-有空写其他的\"><a href=\"#项目完整代码-有空写其他的\" class=\"headerlink\" title=\"项目完整代码(有空写其他的)\"></a>项目完整代码(有空写其他的)</h1><blockquote>\n<p>这个项目中仅有一个训练一轮的模型(所以仅供看看).如果有需要还是要训练多点,但是前提当然是你有一个好的机器或者服务器,如果有信用卡也可以选择取Github Cloud 或者AWS上选择免费的服务器来帮你训练(时间会很长)</p>\n</blockquote>\n<p><a href=\"https://pan.baidu.com/s/1D18ZwROdqyilBuy6EU4xLA\" title=\"chatbot2\" target=\"_blank\" rel=\"noopener\">chatbot2</a></p>\n","text":"Seq2Seq关于Seq2Seq模型,看如下博客即可,针对的视频讲解可以去BliBli搜相关视频,基本一致. 深度学习的seq2seq模型注意要看下注意力机制tensorflow 基础项目完整代码(有空写其他的)这个项目中仅有一个训练一轮的模型(所以仅供看看).如果有需要还是要训","link":"","raw":null,"photos":[],"categories":[{"name":"Python","slug":"Python","count":41,"path":"api/categories/Python.json"},{"name":"机器学习","slug":"Python/机器学习","count":2,"path":"api/categories/Python/机器学习.json"},{"name":"聊天机器人内核","slug":"Python/机器学习/聊天机器人内核","count":1,"path":"api/categories/Python/机器学习/聊天机器人内核.json"}],"tags":[{"name":"Python","slug":"Python","count":65,"path":"api/tags/Python.json"},{"name":"机器学习","slug":"机器学习","count":4,"path":"api/tags/机器学习.json"},{"name":"聊天机器人内核","slug":"聊天机器人内核","count":3,"path":"api/tags/聊天机器人内核.json"}]},{"title":"聊天室内核从0开始 - 1 前置知识与NLP","slug":"type-2","date":"2019-01-01T13:02:42.000Z","updated":"2019-07-03T13:51:36.863Z","comments":true,"path":"api/articles/type-2.json","excerpt":"","keywords":null,"cover":"http://be-sunshine.cn/wp-content/uploads/2019/01/1234352-13d969531284a9f9.png","content":"<blockquote>\n<p>最后更新于2019/1/6</p>\n</blockquote>\n<h1 id=\"前置知识\"><a href=\"#前置知识\" class=\"headerlink\" title=\"前置知识\"></a>前置知识</h1><h2 id=\"TensorFlow\"><a href=\"#TensorFlow\" class=\"headerlink\" title=\"TensorFlow\"></a>TensorFlow</h2><blockquote>\n<ol>\n<li>张量(Tensor)</li>\n<li>图(Flow-&gt;Graph)</li>\n<li>会话(Session)</li>\n</ol>\n</blockquote>\n<h3 id=\"张量-Tensor\"><a href=\"#张量-Tensor\" class=\"headerlink\" title=\"张量(Tensor)\"></a>张量(Tensor)</h3><blockquote>\n<p>类似于矩阵,一维的张量叫做向量</p>\n</blockquote>\n<h3 id=\"计算图-Graph\"><a href=\"#计算图-Graph\" class=\"headerlink\" title=\"计算图(Graph)\"></a>计算图(Graph)</h3><blockquote>\n<p>TensorFlow的计算图的组成和数据结构中的图不同.</p>\n<blockquote>\n<ol>\n<li>图的节点: op-&gt;即操作.</li>\n<li>图的边: 即数据流,此处的数据流就是上述张量.</li>\n</ol>\n</blockquote>\n</blockquote>\n<h3 id=\"会话-Session\"><a href=\"#会话-Session\" class=\"headerlink\" title=\"会话(Session)\"></a>会话(Session)</h3><blockquote>\n<p>在TensorFlow中,要想启动一个图的前提是要先创建一个会话,Ts所有对图的操作,都必须在会话中进行.</p>\n</blockquote>\n<h3 id=\"模型训练一般流程\"><a href=\"#模型训练一般流程\" class=\"headerlink\" title=\"模型训练一般流程\"></a>模型训练一般流程</h3><p>document.write(“graph TD\\nA(开始) –&gt; B(定义数据集)\\nB –&gt; C(定义模型)\\nC –&gt;D(编写并训练模型)\\nD –&gt;E(模型测试)\\n”);</p>\n<h2 id=\"Android\"><a href=\"#Android\" class=\"headerlink\" title=\"Android\"></a>Android</h2><h3 id=\"模拟机替代AVG\"><a href=\"#模拟机替代AVG\" class=\"headerlink\" title=\"模拟机替代AVG\"></a>模拟机替代AVG</h3><p>为了解决Android Studio使用AVG虚拟机时会导致内存占用过大的问题. 这里我们选用网易旗下的MuMu模拟器.</p>\n<h4 id=\"步骤\"><a href=\"#步骤\" class=\"headerlink\" title=\"步骤\"></a>步骤</h4><p>document.write(“graph TD\\nA(开启MuMu模拟器) –&gt; B(进入到MuMu根目录的上层目录)\\nB –&gt; C(找到vmonitorbin)\\nC –&gt;D(在cmd中进入到上述地址-先进入盘符)\\nD –&gt;E(输入adb_server.exe connect port-如7555)\\n”);</p>\n<p>注意:这个必须在Android Studio开启时连接才可以,如果出现Empty host name,多连接几次就可以了. 使用这个方法就可以有效减少内存的占用问题.</p>\n<h2 id=\"神经网络\"><a href=\"#神经网络\" class=\"headerlink\" title=\"神经网络\"></a>神经网络</h2><h3 id=\"神经网络相关学习视频\"><a href=\"#神经网络相关学习视频\" class=\"headerlink\" title=\"神经网络相关学习视频\"></a>神经网络相关学习视频</h3><p><a href=\"https://www.icourse163.org/learn/DA-1002183004?tid=1002301011#/learn/content\" target=\"_blank\" rel=\"noopener\">https://www.icourse163.org/learn/DA-1002183004?tid=1002301011#/learn/content</a> 因为其中涉及内容过多,且基础,故我在此仅讲解几个比较生疏难懂的概念,而不做展开,其基础推荐在学号高数和概率论以及线性代数这三门课程以后再进行展开(也可以不需要,但理解上会有些困难).</p>\n<h3 id=\"梯度下降\"><a href=\"#梯度下降\" class=\"headerlink\" title=\"梯度下降\"></a>梯度下降</h3><blockquote>\n<p>在了解梯度下降前你需要知道</p>\n<blockquote>\n<ol>\n<li>计算图的节点是简单的操作</li>\n<li>高数求导中的链式求导法</li>\n<li>既然它们的节点时简单的运算,那么就可以很方便地使用链式求导法则对其进行求导 &gt; 比如 Y=Z+b,Z=b+2 dY/db=dZ/db+d(b)/db</li>\n</ol>\n</blockquote>\n<p>则如下图所示 <img src=\"http://be-sunshine.cn/wp-content/uploads/2019/01/1234352-13d969531284a9f9.png\" alt> 计算图是正向传播的,而计算图中的反向计算梯度是反向计算的,即函数的上升方向. 故反向传播以后计算出来的梯度只需要取负即是梯度下降的方向了.</p>\n</blockquote>\n<p>以上基础知识熟悉以后可以看这篇文章: <a href=\"https://www.jianshu.com/p/c7e642877b0e\" title=\"深入浅出,梯度下降法及其实现\" target=\"_blank\" rel=\"noopener\">深入浅出,梯度下降法及其实现</a></p>\n<h3 id=\"向量化\"><a href=\"#向量化\" class=\"headerlink\" title=\"向量化\"></a>向量化</h3><blockquote>\n<p>这个不难理解,即计算图中数据流都是向量,大大缩短计算时间.</p>\n</blockquote>\n<h3 id=\"Python广播机制\"><a href=\"#Python广播机制\" class=\"headerlink\" title=\"Python广播机制\"></a>Python广播机制</h3><blockquote>\n<p>举几个例子吧,具体在实践中总结,或查阅相关DOC.</p>\n</blockquote>\n<pre><code>A = numpy.array([1,2,3])\nresult = A + 100\nprint(result)\n\n输出: [101 102 103]</code></pre><h3 id=\"激活函数\"><a href=\"#激活函数\" class=\"headerlink\" title=\"激活函数\"></a>激活函数</h3><blockquote>\n<p>计算图的每层节点将上一层的输出作为本层的输入,如果没有激活函数,那么最终的结果等效于F(x)=x即线性函数. 而激活函数则是在层与层间添加上一个激活函数,使之并不完全作为线性函数来传递数据(即进入什么数据,出来就一定是唯一结果) 即由激活函数判断是否输出</p>\n</blockquote>\n<h3 id=\"神经元模型-参考书籍-西瓜书\"><a href=\"#神经元模型-参考书籍-西瓜书\" class=\"headerlink\" title=\"神经元模型(参考书籍-西瓜书)\"></a>神经元模型(参考书籍-西瓜书)</h3><p><img src=\"http://be-sunshine.cn/wp-content/uploads/2019/01/20180507224752660.png\" alt></p>\n<blockquote>\n<p>在生物神经网络中,，每个神经元与其他神经元相连,当它”兴奋”时,就会向相连的神经元发送化学物质,从而改变这些神经元内的电位,如果某神经元的电位超过一个”阈值”,那么他就会被激活,即”兴奋”起来,向其他神经元发送化学物质. 现在最常用的组成神经网络的节点神经元模型是M-P神经元模型.在这个模型中,神经元接收到来自其它n个神经元传递过来的输入信号,这些输入信号通过带权重的连接进行传递,神经元接收到的总输入值将与神经元的阈值进行比较,然后通过”激活函数”处理以产生神经元的输出.</p>\n</blockquote>\n<h3 id=\"神经网络分类\"><a href=\"#神经网络分类\" class=\"headerlink\" title=\"神经网络分类\"></a>神经网络分类</h3><blockquote>\n<p>大部分在上述的视频中都有系统介绍,这里我只做总结</p>\n</blockquote>\n<h4 id=\"感知机与深层神经网络\"><a href=\"#感知机与深层神经网络\" class=\"headerlink\" title=\"感知机与深层神经网络\"></a>感知机与深层神经网络</h4><p><img src=\"http://be-sunshine.cn/wp-content/uploads/2019/01/p.png\" alt></p>\n<blockquote>\n<p>即 输入层 -&gt; 隐藏层 -&gt; 输出层</p>\n</blockquote>\n<h4 id=\"BP神经网络\"><a href=\"#BP神经网络\" class=\"headerlink\" title=\"BP神经网络\"></a>BP神经网络</h4><blockquote>\n<p>BP网络即前向传播+反向传播来更新偏置. 特点:</p>\n<blockquote>\n<p>1-可以通过逐层信息传递到最后的输出. 2-沿着一条直线计算,直到最后一层,求出计算结果. 3-包含输入层、输出层和隐藏层,其目的是实现从输入到输出的映射. 4-一般包含多层,并且层与层之间是全连接的,不存在同层和跨层连接.</p>\n</blockquote>\n</blockquote>\n<h4 id=\"循环神经网络RNN和LSTM\"><a href=\"#循环神经网络RNN和LSTM\" class=\"headerlink\" title=\"循环神经网络RNN和LSTM\"></a>循环神经网络RNN和LSTM</h4><blockquote>\n<p>这类计算图是针对于成序列的数据的.</p>\n<blockquote>\n<p>类似于造句,造音乐等.如果一个序列过长,则会导致可能在计算后面序列的时候将前面序列的影响变低.从而导致序列无法有效处理”长期依赖”的问题.</p>\n</blockquote>\n</blockquote>\n<h5 id=\"RNN\"><a href=\"#RNN\" class=\"headerlink\" title=\"RNN\"></a>RNN</h5><h6 id=\"前向传播\"><a href=\"#前向传播\" class=\"headerlink\" title=\"前向传播\"></a>前向传播</h6><blockquote>\n<p>普通的CNN模型即上述的神经网络模型,而NLP遵守的规则一般为对于一个句子的分析,每个单词的分析,如果采取上述的线性模型可能会导致语言没有一点逻辑,即 “我/爱/你” - 则针对我输出A,针对爱输出B,针对你输出C 得到结果ABC. 而RNN最大的特点在于其可记忆性.什么叫可记忆性呢?即上一个单词的结果要传递给下一层,使下一个单词运算出的结果可以结合上一个单词以及当前的单词一起得出最后的结果.</p>\n</blockquote>\n<p><img src=\"http://be-sunshine.cn/wp-content/uploads/2019/01/1042406-20170306142253375-175971779.png\" alt></p>\n<blockquote>\n<p>如上图,右侧是拆分后的RNN,x(i)代表的是第i个单词,而计算后的h(i)会传递给h(i+1),y(i)是由上一层计算出来的词向量. 数学公式表达为:</p>\n<blockquote>\n<p>h(i)=g(w*h(i-1)+w*x(i)+bh) g(激活函数)一般选为tanh/Relu y(i)=g(w*h(i)+by) g一般选择sigmod/softmax</p>\n</blockquote>\n<p>优化: 因为词向量的行是相同的,所以将列拼在一起即可.</p>\n<blockquote>\n<p>h(i)=g(w*[h(i-1),x(i)]+bh)</p>\n</blockquote>\n</blockquote>\n<h6 id=\"反向传播\"><a href=\"#反向传播\" class=\"headerlink\" title=\"反向传播\"></a>反向传播</h6><blockquote>\n<p>反向传播作用依然是：减少误差,计算lost函数. 用倒数来计算某一个节点队最终结果的影响程度.训练完后,取平均值(大概,这点我没太仔细看).</p>\n</blockquote>\n<p>一个比较通俗易懂的链接: <a href=\"https://blog.csdn.net/shaomingliang499/article/details/50587300\" title=\" 一步一步教你反向传播\" target=\"_blank\" rel=\"noopener\">一步一步教你反向传播</a></p>\n<h6 id=\"RNN的几种类型\"><a href=\"#RNN的几种类型\" class=\"headerlink\" title=\"RNN的几种类型\"></a>RNN的几种类型</h6><p><img src=\"http://be-sunshine.cn/wp-content/uploads/2019/01/20180719232051968.jpg\" alt></p>\n<h5 id=\"LSTM\"><a href=\"#LSTM\" class=\"headerlink\" title=\"LSTM\"></a>LSTM</h5><blockquote>\n<p>再清楚了RNN以后,LSTM其实就是基于RNN的一个变种.</p>\n<blockquote>\n<p>因为RNN实际应用中无法解决长效记忆的问题,所以催生出了LSTM这一模型.放一个简单的视频可以看下.</p>\n</blockquote>\n<p><a href=\"https://www.bilibili.com/video/av15998549?from=search&seid=17651800282007333668\" title=\"什么是 LSTM RNN 循环神经网络 ?\" target=\"_blank\" rel=\"noopener\">什么是 LSTM RNN 循环神经网络 ?</a></p>\n</blockquote>\n<h2 id=\"NLP\"><a href=\"#NLP\" class=\"headerlink\" title=\"NLP\"></a>NLP</h2><h3 id=\"介绍\"><a href=\"#介绍\" class=\"headerlink\" title=\"介绍\"></a>介绍</h3><blockquote>\n<p>自然语言处理,探索如何处理及运用自然语言,即让电脑懂人类的语言. 包含文本分析、信息检索、词性标注、问答系统等.</p>\n</blockquote>\n<ol>\n<li><p>词法分析</p>\n<blockquote>\n<p>分词技术、词性标注(名词n,形容词a,副词d,人称代词rr,动词v…)、命名实体识别、词义消歧</p>\n</blockquote>\n</li>\n<li><p>句法分析</p>\n</li>\n<li><p>语义分析</p>\n</li>\n</ol>\n<h3 id=\"分词技术\"><a href=\"#分词技术\" class=\"headerlink\" title=\"分词技术\"></a>分词技术</h3><blockquote>\n<p>中科院分词系统(nlpir): <a href=\"http://ictclas.nlpir.org/nlpir/\" title=\"语义分词系统\" target=\"_blank\" rel=\"noopener\">中科院语义分词系统</a></p>\n</blockquote>\n<h3 id=\"命名实体识别\"><a href=\"#命名实体识别\" class=\"headerlink\" title=\"命名实体识别\"></a>命名实体识别</h3><blockquote>\n<p>即分词方法. 命名实体识别（Named Entity Recognition，简称NER），又称作“专名识别”，是指识别文本中具有特定意义的实体，主要包括人名、地名、机构名、专有名词等。 一般分为两种方法</p>\n<blockquote>\n<p>基于规则和词典的方法.</p>\n</blockquote>\n</blockquote>\n<p>document.write(“graph TD\\n\\nC(基于统计的方法)\\nC –&gt;D[隐马尔可夫模型]\\nC –&gt;E[较大熵]\\nC –&gt;F[支持向量机]\\nC –&gt;G[条件随机场]\\n”);</p>\n<h3 id=\"朴素贝叶斯\"><a href=\"#朴素贝叶斯\" class=\"headerlink\" title=\"朴素贝叶斯\"></a>朴素贝叶斯</h3><p>即概率论中的贝叶斯概型.</p>\n<blockquote>\n<p>在为序列定型中的用法:</p>\n<blockquote>\n<p>如: 分次以后判断每个单词是垃圾邮件的可能性大小,再用朴素贝叶斯计算出该邮件是垃圾邮件的概率.</p>\n</blockquote>\n</blockquote>\n<h3 id=\"马尔科夫过程\"><a href=\"#马尔科夫过程\" class=\"headerlink\" title=\"马尔科夫过程\"></a>马尔科夫过程</h3><blockquote>\n<p>其实我也没搞懂意义在哪~ (1)独立随机过程为马尔可夫过程。 (2)独立增量过程为马尔可夫过程：没{X(t)，t∈[0，+∞)}为一独立增量过程，且有P(X(0)=x0)=1，x0为常数，则X(t)为马尔可夫过程。 (3)泊松过程为马尔可夫过程。 (4)维纳过程为马尔可夫过程。 (5)质点随机游动过程为马尔可夫过程。 即下一时刻的状态只依赖于上一时刻,而与上一时刻以前无关.</p>\n</blockquote>\n<h3 id=\"语料的处理方法\"><a href=\"#语料的处理方法\" class=\"headerlink\" title=\"语料的处理方法\"></a>语料的处理方法</h3><ol>\n<li>数据清洗(去掉无意义的标签,url,符号等)</li>\n<li>分词、大小写转换、添加句首句尾、词性标注.</li>\n<li>统计词频、抽取文本特征、特征选择、计算特征权重、归一化</li>\n<li>划分训练集、测试集（先分几份,然后7-3划分）</li>\n</ol>\n<h1 id=\"聊天室内核从0开始-–-2-处理语料库\"><a href=\"#聊天室内核从0开始-–-2-处理语料库\" class=\"headerlink\" title=\"聊天室内核从0开始 – 2 处理语料库\"></a>聊天室内核从0开始 – 2 处理语料库</h1><p><a href=\"http://be-sunshine.cn/index.php/2019/01/04/type-3/\" target=\"_blank\" rel=\"noopener\">http://be-sunshine.cn/index.php/2019/01/04/type-3/</a></p>\n","text":"最后更新于2019/1/6前置知识TensorFlow张量(Tensor)图(Flow-&gt;Graph)会话(Session)张量(Tensor)类似于矩阵,一维的张量叫做向量计算图(Graph)TensorFlow的计算图的组成和数据结构中的图不同.图的节点: op-&gt","link":"","raw":null,"photos":[],"categories":[{"name":"Android","slug":"Android","count":5,"path":"api/categories/Android.json"},{"name":"NLP","slug":"Android/NLP","count":1,"path":"api/categories/Android/NLP.json"},{"name":"TensorFlow","slug":"Android/NLP/TensorFlow","count":1,"path":"api/categories/Android/NLP/TensorFlow.json"},{"name":"机器学习","slug":"Android/NLP/TensorFlow/机器学习","count":1,"path":"api/categories/Android/NLP/TensorFlow/机器学习.json"},{"name":"聊天机器人内核","slug":"Android/NLP/TensorFlow/机器学习/聊天机器人内核","count":1,"path":"api/categories/Android/NLP/TensorFlow/机器学习/聊天机器人内核.json"}],"tags":[{"name":"Android","slug":"Android","count":5,"path":"api/tags/Android.json"},{"name":"机器学习","slug":"机器学习","count":4,"path":"api/tags/机器学习.json"},{"name":"聊天机器人内核","slug":"聊天机器人内核","count":3,"path":"api/tags/聊天机器人内核.json"},{"name":"NLP","slug":"NLP","count":1,"path":"api/tags/NLP.json"},{"name":"TensorFlow","slug":"TensorFlow","count":1,"path":"api/tags/TensorFlow.json"}]},{"title":"聊天室内核从0开始 – 2 处理语料库","slug":"type-3","date":"2019-01-04T12:48:01.000Z","updated":"2019-07-03T13:51:36.863Z","comments":true,"path":"api/articles/type-3.json","excerpt":"","keywords":null,"cover":null,"content":"<blockquote>\n<p>最后更新于2019/1/4</p>\n</blockquote>\n<h1 id=\"处理语料库\"><a href=\"#处理语料库\" class=\"headerlink\" title=\"处理语料库\"></a>处理语料库</h1><h2 id=\"简单处理提问与应答\"><a href=\"#简单处理提问与应答\" class=\"headerlink\" title=\"简单处理提问与应答\"></a>简单处理提问与应答</h2><blockquote>\n<p>这里我们本来可以用jieba或者其他的一些库来帮忙分词,但是因为网上存在已经分好词的语料库,所以可以省略这一步. 至于语料库,可以在github上直接搜索语料库即可以搜到. 一般语料库如下:</p>\n</blockquote>\n<pre><code>E\nM 呵/呵\nM 是/王/若/猫/的/。\nE\nM 不/是\nM 那/是/什/么/？\nE\nM 怎/么/了\nM 我/很/难/过/，/安/慰/我/~\nE\nM 开/心/点/哈/,/一/切/都/会/好/起/来\nM 嗯/ /会/的</code></pre><p>其中E代表是下一组应答的开始,M代表的是一句话. 可以认为是一问一答.</p>\n<h2 id=\"将一个句子编码化\"><a href=\"#将一个句子编码化\" class=\"headerlink\" title=\"将一个句子编码化\"></a>将一个句子编码化</h2><blockquote>\n<p>因为我们如果企图对一个句子进行判断和操作时,我们需要将这串句子编码化成为一组数字更为方便,且占内存更少. 当然,我们用这组句子编码成数字以后也可以重新根据数字编码回字符.</p>\n</blockquote>\n<p>举个栗子: map[3]=’a’,map[2]=’b’ 则 32 = ab,且 ba = 23</p>\n<blockquote>\n<p>Python的一个语法糖:</p>\n<blockquote>\n<p>[2]*2=[2,2]</p>\n</blockquote>\n<p>2019/1/9 日更新-完善注释</p>\n</blockquote>\n<pre><code>import numpy as np\n\n# 句子编码化\nclass WordSequence(object):\n    #标注TAG\n    PAD_TAG=&apos;&lt;pad&gt;&apos;\n    UNK_TAG=&apos;&lt;unk&gt;&apos; # 未识别\n    START_TAG=&apos;&lt;s&gt;&apos;\n    END_TAG=&apos;&lt;/s&gt;&apos;\n\n    PAD=0\n    UNK=1\n    START=2\n    END=3\n\n    # 初始化标签\n    def __init__(self):\n        self.dict = {\n            WordSequence.PAD_TAG: WordSequence.PAD,\n            WordSequence.UNK_TAG: WordSequence.UNK,\n            WordSequence.START_TAG: WordSequence.START,\n            WordSequence.END_TAG: WordSequence.END\n        }\n        # 是否训练过了\n        self.fited=False\n\n    # 将word的词性转换为下标\n    def to_index(self,word):\n        assert self.fited,&apos;WordSequence尚未进行fit操作&apos;\n        # 如果有,返回下标\n        if word in self.dict:\n            return self.dict[word]\n        # 没有,返回UNKnow\n        return WordSequence.UNK\n\n    def to_word(self,index):\n        assert self.fited,&apos;WordSequence尚未进行fit操作&apos;\n        # 遍历dict,找到就返回value\n        for k,v in self.dict.items():\n            if v==index:\n                return k\n        # 否则返回不知道\n        return WordSequence.UNK_TAG\n\n    def size(self):\n        assert self.fited, &apos;WordSequence尚未进行fit操作&apos;\n        return len(self.dict) + 1\n\n    def __len__(self):\n        return self.size()\n\n    # 对数据进行处理\n    def fit(self,sentences,min_count=5,max_count=None,max_features=None):\n        assert not self.fited, &apos;WordSequence只能进行一次fit&apos;\n\n        count={}\n        # 遍历所有句子\n        for sentence in sentences:\n            arr=list(sentence)\n            # 统计词频\n            for a in arr:\n                if a not in count:\n                    count[a]=0\n                count[a]+=1\n        # 只统计词频大于最小值的\n        if min_count is not None:\n            count={k:v for k,v in count.items() if v&gt;=min_count}\n        # 仅统计词频小于最大值的\n        if max_count is not None:\n            count={k:v for k,v in count.items() if v&lt;=max_count}\n\n        # 如果有特征值数的限制,比如[1,2,3]max_features=2,则\n        # 需要用的是[1,2]\n        if isinstance(max_features,int):\n            # list(dict)=[(key,value),...]\n            count = sorted(list(count.items()),key=lambda x:x[1])\n            if max_features is not None and len(count) &gt; max_features:\n                count = count[-int(max_features):]# 从尾部向前\n            # 这个以及下面那个类似于前向星式存图法里的\n            # 下标递增式存法,即加入一个元素,该元素下标\n            # 变成当前已存在的元素个数\n            # 就是把count中的key作为dict的key\n            # 在dict中的下标作为dict的value\n            for w,_ in count:\n                self.dict[w]=len(self.dict)\n        else:\n            for w in sorted(count.keys()):\n                self.dict[w]=len(self.dict)\n\n        # 处理完成\n        self.fited=True\n\n    # 序列成数列\n    def transform(self, sentence, max_len=None):\n        assert self.fited, &apos;WordSequence尚未进行fit操作&apos;\n        # PAD -&gt; 填充标签,先填充本来的句子长度所有元素为PAD\n        # 如: [&apos;&lt;PAD&gt;&apos;,&apos;&lt;PAD&gt;&apos;...]\n        if max_len is not None:\n            r = [self.PAD] * max_len\n        else:\n            r = [self.PAD] * len(sentence)\n\n        for index, a in enumerate(sentence):\n            if max_len is not None and index &gt;= len(r):\n                break\n            r[index] = self.to_index(a)\n\n        return np.array(r)\n\n    # 序列转回字母\n    def inverse_transform(self, indices, ignore_pad=False, ignore_unk=False, ignore_start=False, ignore_end=False):\n        ret = []\n        for i in indices:\n            word = self.to_word(i)\n            if word == WordSequence.PAD_TAG and ignore_pad:\n                continue\n            if word == WordSequence.UNK_TAG and ignore_unk:\n                continue\n            if word == WordSequence.START_TAG and ignore_start:\n                continue\n            if word == WordSequence.END_TAG and ignore_end:\n                continue\n\n            ret.append(word)\n\n        return ret\n\n\ndef test():\n    ws = WordSequence()\n    ws.fit([[&apos;你&apos;, &apos;好&apos;, &apos;啊&apos;], [&apos;你&apos;, &apos;好&apos;, &apos;哦&apos;], ])\n\n    indice = ws.transform([&apos;我&apos;, &apos;们&apos;, &apos;好&apos;])\n    print(indice)\n\n    back = ws.inverse_transform(indice)\n    print(back)\n\n\nif __name__ == &apos;__main__&apos;:\n    test()</code></pre><blockquote>\n<p>可以发现其实都是一些简单的映射.将每个字符都映射到一个整数上面去.</p>\n<blockquote>\n<p>这样做以后再将其打包成pkl会大大减少占用硬盘: 83MB-&gt;750kb 了解一下</p>\n</blockquote>\n</blockquote>\n<h2 id=\"对于语料中句子的规范化\"><a href=\"#对于语料中句子的规范化\" class=\"headerlink\" title=\"对于语料中句子的规范化\"></a>对于语料中句子的规范化</h2><blockquote>\n<p>这里我们提供三个函数(可自行编码):</p>\n</blockquote>\n<pre><code># 这个函数的作用是在有多个回答的条件下将回答合并起来\ndef make_split(line):\n    if re.match(r&apos;.*([，···?!\\.,!？])$&apos;, &apos;&apos;.join(line)):\n        return []\n\n    return [&apos;, &apos;]\n\n# 是否是一个有意义的句子(这里我们不做规则)\ndef good_line(line):\n    #if len(re.findall(r&apos;[a-zA-Z0-9]&apos;, &apos;&apos;.join(line))) &gt; 2:\n        #return False\n    return True\n\n# 规范化语料,即对于已提取出的预料中的句子进行处理\ndef regular(sen):\n    #sen = re.sub(r&apos;\\.{3,100}&apos;, &apos;···&apos;, sen)\n    #sen = re.sub(r&apos;···{2,100}&apos;, &apos;···&apos;, sen)\n    #sen = re.sub(r&apos;[,]{1,100}&apos;, &apos;，&apos;, sen)\n    #sen = re.sub(r&apos;[\\.]{1,100}&apos;, &apos;。&apos;, sen)\n    #sen = re.sub(r&apos;[\\?]{1,100}&apos;, &apos;？&apos;, sen)\n    #sen = re.sub(r&apos;[!]{1,100}&apos;, &apos;！&apos;, sen)\n\n    return sen</code></pre><blockquote>\n<p>可以发现我基本都注释掉了,因为小黄鸡的语料库就是标准的一问一答,不是自然地语料库(微信随便提取的那类),所以不需要过多的处理.</p>\n</blockquote>\n<h2 id=\"打包成pkl文件\"><a href=\"#打包成pkl文件\" class=\"headerlink\" title=\"打包成pkl文件\"></a>打包成pkl文件</h2><blockquote>\n<p>首先说一下pkl文件:</p>\n<blockquote>\n<p>pkl文件是Python运行时产生的数据序列化后存储下来的文件格式,类似于其他语言的序列化.方便以后的继续使用和读取.</p>\n</blockquote>\n</blockquote>\n<p>具体的处理语料库和打包(一问一答Tuple)代码如下:</p>\n<pre><code># -*- coding:utf-8 -*-\n\nimport re\nimport pickle\nimport sys\nfrom tqdm import tqdm\n\n\ndef make_split(line):\n    if re.match(r&apos;.*([，···?!\\.,!？])$&apos;, &apos;&apos;.join(line)):\n        return []\n\n    return [&apos;, &apos;]\n\n# 是否是一个有意义的句子(这里我们不做规则)\ndef good_line(line):\n    #if len(re.findall(r&apos;[a-zA-Z0-9]&apos;, &apos;&apos;.join(line))) &gt; 2:\n        #return False\n    return True\n\n# 规范化语料\ndef regular(sen):\n    #sen = re.sub(r&apos;\\.{3,100}&apos;, &apos;···&apos;, sen)\n    #sen = re.sub(r&apos;···{2,100}&apos;, &apos;···&apos;, sen)\n    #sen = re.sub(r&apos;[,]{1,100}&apos;, &apos;，&apos;, sen)\n    #sen = re.sub(r&apos;[\\.]{1,100}&apos;, &apos;。&apos;, sen)\n    #sen = re.sub(r&apos;[\\?]{1,100}&apos;, &apos;？&apos;, sen)\n    #sen = re.sub(r&apos;[!]{1,100}&apos;, &apos;！&apos;, sen)\n\n    return sen\n\n# 这样设置的意思是无限制\ndef main(limit=99999, x_limit=1, y_limit=1):\n    from word_sequence import WordSequence\n    print(&apos;extract lines&apos;)\n    fp=open(&quot;xiaohuangji.conv&quot;,&apos;r&apos;,errors=&apos;ignore&apos;,encoding=&apos;utf-8&apos;)\n\n    groups=[]\n    group=[]\n\n    # 提取出所有问答组\n    for line in tqdm(fp):\n        if line.startswith(&apos;M &apos;):\n            line=line.replace(&apos;\\n&apos;,&apos;&apos;)\n            if &apos;/&apos; in line:\n                line = line[2:].split(&apos;/&apos;)\n            else:\n                line=list(line[2:])\n            line=line[:-1]\n\n            group.append(list(regular(&apos;&apos;.join(line))))\n        else:\n            if group:\n                groups.append(group)\n                group = []\n    if group:\n        groups.append(group)\n        group = []\n\n    print(&apos;extract group&apos;)\n\n    x_data = []\n    y_data = []\n    # 将问与答分开\n    for group in tqdm(groups):\n        for i,line in  enumerate(group):\n            last_line=None\n            # last_line是上一句\n            if i&gt;0:\n                last_line = group[i-1]\n                if not good_line(last_line):\n                    last_line = None\n            if i&lt;len(group)-1:\n                next_line=group[i+1]\n                if not good_line(next_line):\n                    next_line=None\n            # 如果有下一句\n            if not last_line:\n                x_data.append(line)\n                y_data.append(next_line)\n\n        #print(len(x_data), len(y_data))\n\n    print(len(x_data), len(y_data))\n    # 构建问答,测试前20个\n    for ask,answer in zip(x_data[:20],y_data[:20]):\n        print(&apos;&apos;.join(ask))\n        print(&apos;&apos;.join(answer))\n        print(&apos;-&apos; * 20)\n\n    # 生成pkl文件\n    data=list(zip(x_data,y_data))\n\n    data=[\n        (x,y) for x,y in data if limit&gt;len(x) &gt;=x_limit and limit &gt; len(y) &gt;= y_limit\n    ]\n\n    # 打包成pkl\n    x_data, y_data = zip(*data)\n    ws_input = WordSequence()\n    ws_input.fit(x_data + y_data)\n    print(&apos;dump&apos;)\n    pickle.dump(\n        (x_data, y_data), open(&apos;chatbot.pkl&apos;, &apos;wb&apos;))\n    pickle.dump(ws_input, open(&apos;ws.pkl&apos;, &apos;wb&apos;))\n    print(&apos;done&apos;)\n\nif __name__ == &apos;__main__&apos;:\n    main()</code></pre><h1 id=\"聊天室内核从0开始-–-3-Seq2Seq\"><a href=\"#聊天室内核从0开始-–-3-Seq2Seq\" class=\"headerlink\" title=\"聊天室内核从0开始 – 3 Seq2Seq\"></a>聊天室内核从0开始 – 3 Seq2Seq</h1><p><a href=\"http://be-sunshine.cn/index.php/2019/01/06/seq2seq/\" title=\"聊天室内核从0开始 – 3 Seq2Seq\" target=\"_blank\" rel=\"noopener\">聊天室内核从0开始 – 3 Seq2Seq</a></p>\n","text":"最后更新于2019/1/4处理语料库简单处理提问与应答这里我们本来可以用jieba或者其他的一些库来帮忙分词,但是因为网上存在已经分好词的语料库,所以可以省略这一步. 至于语料库,可以在github上直接搜索语料库即可以搜到. 一般语料库如下:EM 呵/呵M 是/王/若/猫/的/","link":"","raw":null,"photos":[],"categories":[{"name":"Python","slug":"Python","count":41,"path":"api/categories/Python.json"},{"name":"聊天机器人内核","slug":"Python/聊天机器人内核","count":1,"path":"api/categories/Python/聊天机器人内核.json"}],"tags":[{"name":"Python","slug":"Python","count":65,"path":"api/tags/Python.json"},{"name":"聊天机器人内核","slug":"聊天机器人内核","count":3,"path":"api/tags/聊天机器人内核.json"}]}]}