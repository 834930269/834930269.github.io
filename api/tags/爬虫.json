{"name":"爬虫","slug":"爬虫","count":4,"postlist":[{"title":"股票数据定向爬虫","slug":"gupiaodingxiangpachong","date":"2017-09-07T12:05:07.000Z","updated":"2019-07-03T13:51:36.852Z","comments":true,"path":"api/articles/gupiaodingxiangpachong.json","excerpt":"","keywords":null,"cover":null,"content":"<p>累了…直接撩代码</p>\n<pre><code>import requests\nfrom bs4 import BeautifulSoup\nimport traceback\nimport re\n\ndef getHTMLText(url):\n    try:\n        r=requests.get(url)\n        r.raise_for_status()\n        r.encoding=r.apparent_encoding\n        return r.text\n    except:\n        return &quot;&quot;\n\ndef getStockList(lst,stockURL):\n    html=getHTMLText(stockURL)\n    soup=BeautifulSoup(html,&apos;html.parser&apos;)\n    a=soup.find_all(&apos;a&apos;)\n    for i in a:\n        try:\n            href=i.attrs[&apos;href&apos;]\n            lst.append(re.findall(r&apos;[s][hz]\\d{6}&apos;,href)[0])\n        except:\n            continue\n\ndef getStockInfo(lst,stockURL,fpath):\n    for stock in lst:\n        url=stockURL+stock+&quot;.html&quot;\n        html=getHTMLText(url)\n        try:\n            if html==&quot;&quot;:\n                continue\n            infoDict={}\n            soup=BeautifulSoup(html,&apos;html.parser&apos;)\n            stockInfo=soup.find(&apos;div&apos;,attrs={&apos;class&apos;:&apos;stock-bets&apos;})\n\n            name=stockInfo.find_all(attrs={&apos;class&apos;:&apos;bets-name&apos;})[0]\n            infoDict.update({&apos;股票名称&apos;:name.text.split()[0]})\n            keyList=stockInfo.find_all(&apos;dt&apos;)\n            valueList=stockInfo.find_all(&apos;dd&apos;)\n            for i in range(len(keyList)):\n                key=keyList[i].text\n                val=valueList[i].text\n                infoDict[key]=val\n\n            with open(fpath,&apos;a&apos;,encoding=&apos;utf-8&apos;) as f:\n                f.write(str(infoDict)+&apos;\\n&apos;)\n        except:\n            traceback.print_exc()\n            continue\n\nif __name__==&apos;__main__&apos;:\n    stock_list_url = &apos;http://quote.eastmoney.com/stocklist.html&apos;\n    stock_info_url = &apos;https://gupiao.baidu.com/stock/&apos;\n    output_file = &apos;E:\\学习相关\\廖雪峰\\python_study\\库\\第三章\\BaiduStockInfo.txt&apos;\n    slist=[]\n    getStockList(slist,stock_list_url)\n    getStockInfo(slist,stock_info_url,output_file)</code></pre><p>结果(超慢的得得得多多多多多多多,还没爬完…不过应该是解析全文默认编码的问题): 2017-09-07 20:08:30 星期四 :earth_asia:<a href=\"https://github.com/834930269/python_study/blob/master/%E5%BA%93/%E7%AC%AC%E4%B8%89%E7%AB%A0/BaiduStockInfo.txt\" title=\"github: 爬取结果.txt\" target=\"_blank\" rel=\"noopener\">github: 爬取结果.txt</a></p>\n","text":"累了…直接撩代码import requestsfrom bs4 import BeautifulSoupimport tracebackimport redef getHTMLText(url):    try:        r=requests.get(url)       ","link":"","raw":null,"photos":[],"categories":[{"name":"Python","slug":"Python","count":41,"path":"api/categories/Python.json"},{"name":"未分类","slug":"Python/未分类","count":3,"path":"api/categories/Python/未分类.json"},{"name":"爬虫","slug":"Python/未分类/爬虫","count":2,"path":"api/categories/Python/未分类/爬虫.json"}],"tags":[{"name":"Python","slug":"Python","count":65,"path":"api/tags/Python.json"},{"name":"爬虫","slug":"爬虫","count":4,"path":"api/tags/爬虫.json"}]},{"title":"Python 爬取中国大学排名","slug":"python-chut","date":"2017-08-30T10:19:46.000Z","updated":"2019-07-03T13:51:36.852Z","comments":true,"path":"api/articles/python-chut.json","excerpt":"","keywords":null,"cover":"http://be-sunshine.cn/wp-content/uploads/2017/08/17f778160b1fc4d8d2521e6846de2cbe.png","content":"<p>首先是爬取的目的地址: :zap:<a href=\"http://www.zuihaodaxue.cn/zuihaodaxuepaiming2016.html\" title=\"2016中国最好大学排名\" target=\"_blank\" rel=\"noopener\">=&gt;2016中国最好大学排名</a> 这里我们使用的是Python的request库和BeautifulSoup库,IDE用的是Anaconda的Spyder,Python version=3.6. 首先我们现在html中搜索清华大学(废话= =). 结果如下: <img src=\"http://be-sunshine.cn/wp-content/uploads/2017/08/17f778160b1fc4d8d2521e6846de2cbe.png\" alt=\"结果\"> 我们发现结果是在节点为’tbody’中,全部代码中也只有这一个地方有’tbody’.所以我们可以通过查找这个标签,然后再向下查找孩子来找到结果,但是我们知道,他的孩子可能会出现字符串类型,所以我们需要用isinstance(x,bs4.element.Tag),Tag–(标签)来过滤掉其它类型. 于是我们写出A程序:</p>\n<pre><code>#CrawUnivRankingA.py\nimport requests\nfrom bs4 import BeautifulSoup\nimport bs4\n\ndef getHTMLText(url):\n    try:\n        r = requests.get(url, timeout=30)\n        r.raise_for_status()\n        r.encoding = r.apparent_encoding\n        return r.text\n    except:\n        return &quot;&quot;\n\ndef fillUnivList(ulist, html):\n    soup = BeautifulSoup(html, &quot;html.parser&quot;)\n    for tr in soup.find(&apos;tbody&apos;).children:\n        if isinstance(tr, bs4.element.Tag):\n            tds = tr(&apos;td&apos;)\n            ulist.append([tds[0].string, tds[1].string, tds[3].string])\n\ndef printUnivList(ulist, num):\n    print(&quot;{:^10}\\t{:^6}\\t{:^10}&quot;.format(&quot;排名&quot;,&quot;学校名称&quot;,&quot;总分&quot;))\n    for i in range(num):\n        u=ulist[i]\n        print(&quot;{:^10}\\t{:^6}\\t{:^10}&quot;.format(u[0],u[1],u[2]))\n\ndef main():\n    uinfo = []\n    url = &apos;http://www.zuihaodaxue.cn/zuihaodaxuepaiming2016.html&apos;\n    html = getHTMLText(url)\n    fillUnivList(uinfo, html)\n    printUnivList(uinfo, 20) # 20 univs\nmain()</code></pre><p>打印结果: <img src=\"http://be-sunshine.cn/wp-content/uploads/2017/08/807addc82e8f147efd10ff14afadcc03.png\" alt> 可以看出中间的学校缩进不规范,这是因为中文的问题,他是默认按照英文补充空格的. 改进一下,空格用chr(12288)补充:</p>\n<pre><code>#CrawUnivRankingB.py\nimport requests\nfrom bs4 import BeautifulSoup\nimport bs4\n\ndef getHTMLText(url):\n    try:\n        r = requests.get(url, timeout=30)\n        r.raise_for_status()\n        r.encoding = r.apparent_encoding\n        return r.text\n    except:\n        return &quot;&quot;\n\ndef fillUnivList(ulist, html):\n    soup = BeautifulSoup(html, &quot;html.parser&quot;)\n    for tr in soup.find(&apos;tbody&apos;).children:\n        if isinstance(tr, bs4.element.Tag):\n            tds = tr(&apos;td&apos;)\n            ulist.append([tds[0].string, tds[1].string, tds[3].string])\n\ndef printUnivList(ulist, num):\n    tplt = &quot;{0:^10}\\t{1:{3}^10}\\t{2:^10}&quot;\n    print(tplt.format(&quot;排名&quot;,&quot;学校名称&quot;,&quot;总分&quot;,chr(12288)))\n    for i in range(num):\n        u=ulist[i]\n        print(tplt.format(u[0],u[1],u[2],chr(12288)))\n\ndef main():\n    uinfo = []\n    url = &apos;http://www.zuihaodaxue.cn/zuihaodaxuepaiming2016.html&apos;\n    html = getHTMLText(url)\n    fillUnivList(uinfo, html)\n    printUnivList(uinfo, 20) # 20 univs\nmain()</code></pre><p>打印结果: <img src=\"http://be-sunshine.cn/wp-content/uploads/2017/08/a8ee56c437ee19d41e7e53a863779b97.png\" alt> 结束.</p>\n","text":"首先是爬取的目的地址: :zap:=&gt;2016中国最好大学排名 这里我们使用的是Python的request库和BeautifulSoup库,IDE用的是Anaconda的Spyder,Python version=3.6. 首先我们现在html中搜索清华大学(废话= =)","link":"","raw":null,"photos":[],"categories":[{"name":"Python","slug":"Python","count":41,"path":"api/categories/Python.json"},{"name":"爬虫","slug":"Python/爬虫","count":2,"path":"api/categories/Python/爬虫.json"}],"tags":[{"name":"Python","slug":"Python","count":65,"path":"api/tags/Python.json"},{"name":"爬虫","slug":"爬虫","count":4,"path":"api/tags/爬虫.json"}]},{"title":"淘宝竞价爬虫","slug":"taobao-bug","date":"2017-09-06T23:40:58.000Z","updated":"2019-07-03T13:51:36.852Z","comments":true,"path":"api/articles/taobao-bug.json","excerpt":"","keywords":null,"cover":"http://be-sunshine.cn/wp-content/uploads/2017/09/1-1.png","content":"<p>首先我们先观察搜索为书包的链接: 第一页: <code>https://s.taobao.com/search?q=书包&amp;imgfile=&amp;commend=all&amp;ssid=s5-e&amp;search_type=item&amp;sourceId=tb.index&amp;spm=a21bo.50862.201856-taobao-item.1&amp;ie=utf8&amp;initiative_id=tbindexz_20170906</code> 第二页: <code>https://s.taobao.com/search?q=书包imgfile=&amp;commend=all&amp;ssid=s5-e&amp;search_type=item&amp;sourceId=tb.index&amp;spm=a21bo.50862.201856-taobao-item.1&amp;ie=utf8&amp;initiative_id=tbindexz_20170906&amp;bcoffset=4&amp;ntoffset=4&amp;p4ppushleft=1%2C48&amp;s=44</code> 我们可以观察到书包的属性是q,而每页有44个商品,所以页数的属性是s. 所以我们推测当链接为: <code>http://s.taobao.com/search?q=书包&amp;s=44</code> 时,时当前搜索目录的第二页. 首先是getHTMLtext:</p>\n<pre><code>def getHTMLtext(url):\n    try:\n        r=requests.get(url)\n        r.raise_for_status()\n        r.encoding=r.apparent_encoding\n        return r.text\n    except:\n        print(&quot;&quot;)</code></pre><p>然后我们分析代码,比如第一页第一件商品的价钱如下: <img src=\"http://be-sunshine.cn/wp-content/uploads/2017/09/1-1.png\" alt=\"第一件\" title=\"第一件\"> 我们在代码页搜索对应的价格89.00,发现对应的标记是’view_price’: <img src=\"http://be-sunshine.cn/wp-content/uploads/2017/09/2-1.png\" alt=\"89.00\" title=\"89.00\"> 然后是对应的标题,这里有一些问题,因为第一个东西的标题和显示的对不上,显示的是放在’title’标记中,而不知道什么的标题放在’raw_title’中: <img src=\"http://be-sunshine.cn/wp-content/uploads/2017/09/3-1.png\" alt> 然后我们开始编写parse方法:</p>\n<pre><code>def parsePage(til,text):\n    try:\n        r1=re.compile(r&apos;&quot;view_price&quot;:&quot;[\\d.]*&quot;&apos;)\n        r2=re.compile(r&apos;&quot;raw_title&quot;:&quot;.*?&quot;&apos;)\n        lip=re.findall(r1,text)\n        lit=re.findall(r2,text)\n        for i in range(len(lip)):\n            price=eval(lip[i].split(&apos;:&apos;)[1])\n            title=eval(lit[i].split(&apos;:&apos;)[1])\n            til.append([price,title])\n    except:\n        print(&apos;&apos;)</code></pre><p>其中eval可以忽略””这玩意？？？ 最终完成品:</p>\n<pre><code>#-*-coding:utf-8-*-\nimport requests\nimport re\n\ndef getHTMLtext(url):\n    try:\n        r=requests.get(url)\n        r.raise_for_status()\n        r.encoding=r.apparent_encoding\n        return r.text\n    except:\n        print(&quot;&quot;)\n\n\ndef parsePage(til,text):\n    try:\n        r1=re.compile(r&apos;&quot;view_price&quot;:&quot;[\\d.]*&quot;&apos;)\n        r2=re.compile(r&apos;&quot;raw_title&quot;:&quot;.*?&quot;&apos;)\n        lip=re.findall(r1,text)\n        lit=re.findall(r2,text)\n        for i in range(len(lip)):\n            price=eval(lip[i].split(&apos;:&apos;)[1])\n            title=eval(lit[i].split(&apos;:&apos;)[1])\n            til.append([price,title])\n    except:\n        print(&apos;&apos;)\n\ndef printResult(lis):\n    tplt=&quot;{:4}\\t{:8}\\t{:16}&quot;\n    print(tplt.format(&apos;序号&apos;,&apos;价格&apos;,&apos;商品名称&apos;))\n    for i in range(len(lis)):\n        print(tplt.format(i+1,lis[i][0],lis[i][1]))\n        if lis[i][0]==&apos;89.00&apos;:\n            print(&apos;===============&apos;)\n\ndef main():\n    goods=&apos;书包&apos;\n    deepth=3\n    infolist=[]\n    start_url=&apos;https://s.taobao.com/search?q=书包&apos;\n    for i in range(deepth):\n        try:\n            url=start_url+&apos;&amp;s=&apos;+str(i*44)\n            text=getHTMLtext(url)\n            parsePage(infolist,text)\n        except:\n            continue\n    printResult(infolist)\n\nmain()</code></pre><p>结果: <img src=\"http://be-sunshine.cn/wp-content/uploads/2017/09/4-1.png\" alt></p>\n","text":"首先我们先观察搜索为书包的链接: 第一页: https://s.taobao.com/search?q=书包&amp;imgfile=&amp;commend=all&amp;ssid=s5-e&amp;search_type=item&amp;sourceId=tb.index","link":"","raw":null,"photos":[],"categories":[{"name":"Python","slug":"Python","count":41,"path":"api/categories/Python.json"},{"name":"未分类","slug":"Python/未分类","count":3,"path":"api/categories/Python/未分类.json"},{"name":"爬虫","slug":"Python/未分类/爬虫","count":2,"path":"api/categories/Python/未分类/爬虫.json"}],"tags":[{"name":"爬虫","slug":"爬虫","count":4,"path":"api/tags/爬虫.json"},{"name":"UVa","slug":"UVa","count":39,"path":"api/tags/UVa.json"}]},{"title":"词云: Python爬取国际时事","slug":"wordcloud-python","date":"2018-04-01T11:30:24.000Z","updated":"2019-07-03T13:51:36.863Z","comments":true,"path":"api/articles/wordcloud-python.json","excerpt":"","keywords":null,"cover":"http://be-sunshine.cn/wp-content/uploads/2018/04/QQ%E6%88%AA%E5%9B%BE20180401190628.jpg","content":"<h1><span id=\"前置工具\">前置工具</span></h1><blockquote>\n<p>python wordcloud jieba BeautifulSoup matplotlib scipy</p>\n</blockquote>\n<h1><span id=\"第一步-爬取国际时事列表\">第一步: 爬取国际时事列表</span></h1><h2><span id=\"待爬地址-httpmsohucomcr57page1ampv2\">待爬地址: </span></h2><blockquote>\n<p>首先我们可以观察到,每点击列表中的下一页时, page 会加一</p>\n<blockquote>\n<p>然后我们就可以确认,想获取多少条信息,直接替换page属性的值即可</p>\n</blockquote>\n<p>然后我们观察想要爬取的内容:</p>\n</blockquote>\n<p><img src=\"http://be-sunshine.cn/wp-content/uploads/2018/04/QQ%E6%88%AA%E5%9B%BE20180401190628.jpg\" alt></p>\n<h2><span id=\"审查元素\">审查元素:</span></h2><p><img src=\"http://be-sunshine.cn/wp-content/uploads/2018/04/QQ%E6%88%AA%E5%9B%BE20180401190805.jpg\" alt></p>\n<blockquote>\n<p>我们发现文本都是在 div(class=”bd3 pb1”) -&gt; div -&gt; p -&gt; a 标签下的:</p>\n</blockquote>\n<h2><span id=\"编写代码\">编写代码</span></h2><blockquote>\n<p>爬取数据并保存在<strong>data.txt</strong>中:</p>\n</blockquote>\n<pre><code># coding: utf-8\n\nfrom wordcloud import WordCloud\nimport requests\nfrom bs4 import BeautifulSoup\nimport re\n\ndef getHTMLText(url):\n    try:\n        r = requests.get(url)\n        r.raise_for_status()\n        r.encoding = r.apparent_encoding\n        return r.text\n    except:\n        pass\n\ndef has_p_a(tag):\n    pass\n\ndef getWannaData(stockURL,res):\n    html = getHTMLText(stockURL)\n    soup = BeautifulSoup(html,&apos;html.parser&apos;)\n    p = soup.find(&apos;div&apos;,class_=&quot;bd3 pb1&quot;).find_all(&apos;a&apos;)\n    for q in p:\n        res.append(q.text)\n\nres = []\nmaxn = 100\nfor i in range(1,maxn):\n    getWannaData(&apos;http://m.sohu.com/cr/57/?page=&apos;+str(i)+&apos;&amp;v=2&apos;,res)\n\nfile = open(&apos;data.txt&apos;,&apos;a+&apos;)\nfor q in res:\n    file.write(q+&apos;\\n&apos;)</code></pre><blockquote>\n<p>其中maxn是控制爬取多少页的</p>\n</blockquote>\n<h2><span id=\"datatxt-部分内容\">data.txt 部分内容:</span></h2><p><img src=\"http://be-sunshine.cn/wp-content/uploads/2018/04/QQ%E6%88%AA%E5%9B%BE20180401191238.jpg\" alt></p>\n<h1><span id=\"第二步-生成词云\">第二步: 生成词云</span></h1><h2><span id=\"前置\">前置</span></h2><blockquote>\n<p>因为要进行中文分词,所以要用jieba 注意再打开<strong>data.txt</strong>时<strong>编码</strong>问题 还有ttf不能保存在有中文的路径下</p>\n</blockquote>\n<h2><span id=\"背景图片\">背景图片</span></h2><blockquote>\n<p>我们选择 <strong>水伊布.png</strong></p>\n</blockquote>\n<p><img src=\"http://be-sunshine.cn/wp-content/uploads/2018/04/timg.jpg\" alt></p>\n<h2><span id=\"生成词云\">生成词云</span></h2><p><img src=\"http://be-sunshine.cn/wp-content/uploads/2018/04/yb.png\" alt> <strong>容我说一句,在中国相对封闭的网络环境中,已经可以看到世界如此的乱了,全部的词条大部分是消极的…看起来大规模战争结束的时间太久了…(还是说,世界就没有安宁过)</strong></p>\n<blockquote>\n<p>这张图可以找到安倍</p>\n</blockquote>\n<p><img src=\"http://be-sunshine.cn/wp-content/uploads/2018/04/yb2.png\" alt></p>\n","text":"前置工具python wordcloud jieba BeautifulSoup matplotlib scipy第一步: 爬取国际时事列表待爬地址: 首先我们可以观察到,每点击列表中的下一页时, page 会加一然后我们就可以确认,想获取多少条信息,直接替换page属性的值即可","link":"","raw":null,"photos":[],"categories":[{"name":"Python","slug":"Python","count":41,"path":"api/categories/Python.json"},{"name":"爬虫","slug":"Python/爬虫","count":2,"path":"api/categories/Python/爬虫.json"}],"tags":[{"name":"Python","slug":"Python","count":65,"path":"api/tags/Python.json"},{"name":"爬虫","slug":"爬虫","count":4,"path":"api/tags/爬虫.json"}]}]}