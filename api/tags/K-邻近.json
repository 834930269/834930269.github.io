{"name":"K-邻近","slug":"K-邻近","count":1,"postlist":[{"title":"机器学习实战(一) K-近邻","slug":"machine-learning-knn","date":"2018-01-27T14:04:15.000Z","updated":"2019-07-03T13:51:36.861Z","comments":true,"path":"api/articles/machine-learning-knn.json","excerpt":"","keywords":null,"cover":"http://be-sunshine.cn/wp-content/uploads/2018/01/QQ%E6%88%AA%E5%9B%BE20180127214341.jpg","content":"<h1 id=\"K临近算法概述\"><a href=\"#K临近算法概述\" class=\"headerlink\" title=\"K临近算法概述\"></a>K临近算法概述</h1><p>简单地说,k临近算法就是采用不同的特征值之间的距离方法进行分类. 通过数据与数据集间的距离进行分类,以及断定新数据的类别. 这里我们选择使用欧氏距离来当做两点间的距离.</p>\n<h2 id=\"实现KNN算法\"><a href=\"#实现KNN算法\" class=\"headerlink\" title=\"实现KNN算法\"></a>实现KNN算法</h2><h3 id=\"伪码\"><a href=\"#伪码\" class=\"headerlink\" title=\"伪码\"></a>伪码</h3><blockquote>\n<p>对未知类别属性的数据集中的每个点依次执行以下操作</p>\n<blockquote>\n<p>计算已知类别数据集中的点 按照距离递增次序排序 选取与当前点距离最小的k个点 确定前k个点所在的类别的出现频率 返回前k个点出现频率最高的类别作为当前点的预测分类</p>\n</blockquote>\n</blockquote>\n<h3 id=\"实现算法前\"><a href=\"#实现算法前\" class=\"headerlink\" title=\"实现算法前\"></a>实现算法前</h3><p>我们来学习一下需要用到的一些库函数.</p>\n<h4 id=\"numpy\"><a href=\"#numpy\" class=\"headerlink\" title=\"numpy\"></a>numpy</h4><p>1.list转array</p>\n<pre><code>from numpy import *\narray([1,1])</code></pre><p>2.zeros()初始化向量</p>\n<pre><code>import numpy\nfrom numpy import *\na=(3,4)\nzeros(a)\n# 初始化一个3行四列的0矩阵</code></pre><p>3.矩阵操作</p>\n<pre><code>import numpy\nfrom numpy import *\n\nMat = array([[1,2],[3,4]])\n\n# 每行最小\nMat.min(0)\n# 每列最小\nMat.min(1)\n# 每行和\nMat.sum(0)\n# 上面传递的参数都是axis=1 or 0,0代表行,1代表列\n\n# shape返回一个tuple,代表矩阵的行数和列数\nMat.shape</code></pre><p>3.1矩阵排序argsort()</p>\n<pre><code>import numpy\nfrom numpy import *\n\nk = array([1,2,8.5,-1,0])\nt = k.argsort()\n# 输出升序排序后每位数字的下标数组</code></pre><p>输出升序排序后每位数字的下标数组,比如上面那个输出是:</p>\n<pre><code>array([3,4,0,1,2],dtype=int64)\n# 第一个是k[3],第二个是k[4]</code></pre><p>4.tile</p>\n<pre><code>import numpy\nfrom numpy import *\n\n# 有两个参数,第一个参数是初始矩阵,第二个参数是一个tuple,代表\n# 向行拓展次数,以及向列拓展次数,具体调用一下就知道了\ntile([1,2],(1))# 原矩阵\ntile([1,2],(2,2))# 行两倍,列两倍</code></pre><p>5.运算 直接使用运算符号,是相当于每行与每列进行运算. 真正的矩阵运算需要通过库来实现.</p>\n<h4 id=\"数据读取\"><a href=\"#数据读取\" class=\"headerlink\" title=\"数据读取\"></a>数据读取</h4><p>与本例相关的数据集地址: <a href=\"http://be-sunshine.cn:9011/static/file/datingTestSet2.txt\" title=\"datingTestSet2.txt\" target=\"_blank\" rel=\"noopener\">datingTestSet2.txt</a></p>\n<pre><code># 打开数据文件\nfr = open(&apos;datingTestSet2.txt&apos;)\n# 按行读取\narrayOfLines = fr.readlines()\narrayOfLines\n\nfrom numpy import *\nnumberOfLines = len(arrayOfLines)\n# 生成与数据集相同列数的矩阵\nreturnMat = zeros((numberOfLines,3))\nreturnMat\n# 格式化读入,存储到矩阵中\nfor line in arrayOfLines:\n    line = line.strip()\n    print(line.split(&apos;\\t&apos;))\n    print(int(line.split(&apos;\\t&apos;)[-1]))</code></pre><h4 id=\"matplotlib散点图\"><a href=\"#matplotlib散点图\" class=\"headerlink\" title=\"matplotlib散点图\"></a>matplotlib散点图</h4><pre><code>import matplotlib\nimport matplotlib.pyplot as plt\nimport numpy\nfrom numpy import *\n# 生成plt\nfig = plt.figure()\n# 规定最多111个点\nax = fig.add_subplot(111)\n# 创建一个矩阵,第三个代表类别\nMat = array([[1,123,2],[10,256,1],[7,321,3]])\n# 获取类别矩阵\nLabel = Mat[:,2]\n# 第一个参数横坐标,第二个参数纵坐标,第三个参数,颜色矩阵,第三个参数,大小矩阵\nax.scatter(Mat[:,0],Mat[:,1],15.0*Label,15.0*Label)\n# 绘制\nplt.show()</code></pre><h3 id=\"代码实现\"><a href=\"#代码实现\" class=\"headerlink\" title=\"代码实现\"></a>代码实现</h3><h4 id=\"对于代码的解释我都注释在代码中了\"><a href=\"#对于代码的解释我都注释在代码中了\" class=\"headerlink\" title=\"对于代码的解释我都注释在代码中了\"></a>对于代码的解释我都注释在代码中了</h4><pre><code># K-近邻\n&apos;&apos;&apos;\n算法思想:\n\n计算已知类别数据集中的点\n\n按照距离递增次序排序\n\n选取与当前点距离最小的k个点\n\n确定前k个点所在的类别的出现频率\n\n返回前k个点出现频率最高的类别作为当前点的预测分类\n&apos;&apos;&apos;\n\ndef classify0(inX,dataSet,labels,k):\n    &apos;&apos;&apos;\n    k-邻近算法\n    inX:测试数据 - array\n    dataSet:样本数据集 - array\n    labels:标签向量 - array\n    k: 选举前k个 - int\n    &apos;&apos;&apos;\n    # 获取数据集的列数\n    dataSetSize = dataSet.shape[0]\n    # 新建一个矩阵,将测试数据inX复制到每列上,以便计算距离\n    diffMat = tile(inX,(dataSetSize,1)) - dataSet\n    # 对每个指标的距离进行平方\n    sqDiffMat = diffMat**2\n    # 把每个指标的差方相加\n    sqDistance = sqDiffMat.sum(0)\n    # 计算inX与每个点的距离\n    distance = sqDistance**0.5\n    # 升序排序,返回排序后的下标矩阵\n    sortedDistIndicies = distance.argsort()\n\n    # 选择距离最小的k个点\n    classCount = {}\n    for i in range(k):\n        # 选取前k个距离最近的点中的第i个\n        voteIlabel = labels[sortedDistIndicies[i]]\n        # 映射到dict中,其中get的第二个参数是如果不存在的默认值\n        classCount[voteIlabel] = classCount.get(voteIlabel,0) + 1\n    # dict.items()返回一个字典列表(dict_items)类型,即dict的原始插入顺序的list\n    # 可以直接用sorted排序\n    # 升序,其中operator.itemgetter(index)代表按照待排列表的第几个元素排序.\n    # reverse=True即变成了降序\n    sortedClassCount = sorted(classCount.items(),key=operator.itemgetter(1),reverse=True)\n    # 返回分类中频率最高的那个的标签\n    return sortedClassCount[0][0]</code></pre><h3 id=\"可视化分析\"><a href=\"#可视化分析\" class=\"headerlink\" title=\"可视化分析\"></a>可视化分析</h3><pre><code>import numpy\nfrom numpy import *\n# 将测试数据转换为需要的类型\ndef file2matrix(filename):\n    &apos;&apos;&apos;\n    对于datingTestSet2.txt返回值类型\n    returnMat: [里程数,百分比,公升数]\n    --每年获得的飞行常客里程数\n    --玩视频游戏所耗时间百分比\n    --每周消耗的冰淇淋公升数\n    classLabelVector: [标签]\n    --1,2,3分别代表最好,其次,最次\n    &apos;&apos;&apos;\n    fr = open(filename)\n    arrayOLines = fr.readlines()\n    # 得到文件行数\n    numberOfLines = len(arrayOLines)\n    # 新建(文件行数,3列)的0 array\n    returnMat = zeros((numberOfLines,3))\n    classLabelVector = []\n    index = 0\n    # 处理数据\n    for line in arrayOLines:\n        line = line.strip()\n        listFromLine = line.split(&apos;\\t&apos;)\n        # 将数据加入返回的列表中\n        returnMat[index,:] = listFromLine[:3]\n        # 标签列表\n        classLabelVector.append(int(listFromLine[-1]))\n        index+=1\n    return returnMat,classLabelVector\n\nimport matplotlib\nimport matplotlib.pyplot as plt\n\ndatingDataMat,datingLabels = file2matrix(&apos;datingTestSet2.txt&apos;)\n\nfig = plt.figure()\nax = fig.add_subplot(111)\nax.scatter(datingDataMat[:,0],datingDataMat[:,1],15.0*array(datingLabels),15.0*array(datingLabels))\nplt.show()</code></pre><p>最后的结果如下: <a href=\"http://be-sunshine.cn/wp-content/uploads/2018/01/QQ截图20180127214341.jpg\" target=\"_blank\" rel=\"noopener\"><img src=\"http://be-sunshine.cn/wp-content/uploads/2018/01/QQ%E6%88%AA%E5%9B%BE20180127214341.jpg\" alt></a></p>\n<h3 id=\"归一化数值\"><a href=\"#归一化数值\" class=\"headerlink\" title=\"归一化数值\"></a>归一化数值</h3><p>我们可以发现,在数据集中,每种类的数据极差差距都很大,比如飞行常客里程数的极差,和每周消费冰淇淋公升数的极差相距交大. 所以我们尝试将不同的数据集按照相同的区间范围进行计算. 计算公式(和百分制化为150分制的道理一样): <strong>newValue = (OldValue-min)/(max-min)</strong> 其中min和max代表数据集中的最小特征值和最大特征值. 是用这个公式后的数值将统一变成0<del>1或者-1</del>1之间.</p>\n<h4 id=\"代码\"><a href=\"#代码\" class=\"headerlink\" title=\"代码\"></a>代码</h4><pre><code>def autoNorm(dataSet):\n    &apos;&apos;&apos;\n    归一化数值\n    返回值\n    normDataSet:归一化后数值 - array\n    ranges:每类特征极差 - array\n    minVals:每类特征最小值 - array\n    &apos;&apos;&apos;\n    # numpy数组 .min(0)每列最小值\n    # .min(1)每行最小值\n    minVals = dataSet.min(0)\n    maxVals = dataSet.max(0)\n    ranges = maxVals - minVals\n    normDataSet = zeros(shape(dataSet))\n    m = dataSet.shape[0]\n    normDataSet = dataSet - tile(minVals,(m,1))\n    normDataSet = normDataSet/tile(ranges,(m,1))\n    return normDataSet,ranges,minVals</code></pre><h3 id=\"对约会网站的测试\"><a href=\"#对约会网站的测试\" class=\"headerlink\" title=\"对约会网站的测试\"></a>对约会网站的测试</h3><p>最后我们对之前的datingTestSet2.txt进行误差测试 其中hoRatio代表对数据集的测试普及率. 这里用<strong>0.1即1000*0.1=100个</strong>样本数据进行测试.</p>\n<pre><code>def datingClassTest():\n    hoRatio = 0.10\n    datingDataMat,datingLabels = file2matrix(&apos;datingTestSet2.txt&apos;)\n    normMat,ranges,minVals = autoNorm(datingDataMat)\n    m = normMat.shape[0]\n    numTestVecs = int(m*hoRatio)\n    errorCount = 0.0\n    for i in range(numTestVecs):\n        classifierResult = classify0(normMat[i,:],normMat[numTestVecs:m,:],\\\n                                    datingLabels[numTestVecs:m],3)\n        print(&apos;the classifier came back with: %d,the real answer is: %d&apos;\\\n              % (classifierResult,datingLabels[i]))\n        if(classifierResult != datingLabels[i]): errorCount += 1.0\n    print(&apos;the total error rate is: %f&apos; % (errorCount/float(numTestVecs)))\n\ndatingClassTest()</code></pre><p>测试结果:</p>\n<pre><code>the classifier came back with: 1,the real answer is: 3\nthe classifier came back with: 1,the real answer is: 2\nthe classifier came back with: 3,the real answer is: 1\nthe classifier came back with: 1,the real answer is: 1\nthe classifier came back with: 3,the real answer is: 1\nthe classifier came back with: 1,the real answer is: 1\nthe classifier came back with: 1,the real answer is: 3\nthe classifier came back with: 3,the real answer is: 3\nthe classifier came back with: 1,the real answer is: 1\nthe classifier came back with: 3,the real answer is: 3\nthe classifier came back with: 1,the real answer is: 1\nthe classifier came back with: 1,the real answer is: 1\nthe classifier came back with: 1,the real answer is: 2\nthe classifier came back with: 1,the real answer is: 1\nthe classifier came back with: 2,the real answer is: 1\nthe classifier came back with: 3,the real answer is: 1\nthe classifier came back with: 1,the real answer is: 1\nthe classifier came back with: 1,the real answer is: 1\nthe classifier came back with: 3,the real answer is: 2\nthe classifier came back with: 3,the real answer is: 3\nthe classifier came back with: 1,the real answer is: 2\nthe classifier came back with: 3,the real answer is: 1\nthe classifier came back with: 1,the real answer is: 2\nthe classifier came back with: 1,the real answer is: 3\nthe classifier came back with: 1,the real answer is: 2\nthe classifier came back with: 3,the real answer is: 3\nthe classifier came back with: 1,the real answer is: 2\nthe classifier came back with: 3,the real answer is: 3\nthe classifier came back with: 2,the real answer is: 2\nthe classifier came back with: 3,the real answer is: 1\nthe classifier came back with: 3,the real answer is: 3\nthe classifier came back with: 3,the real answer is: 1\nthe classifier came back with: 1,the real answer is: 3\nthe classifier came back with: 1,the real answer is: 1\nthe classifier came back with: 1,the real answer is: 2\nthe classifier came back with: 1,the real answer is: 1\nthe classifier came back with: 1,the real answer is: 1\nthe classifier came back with: 1,the real answer is: 2\nthe classifier came back with: 3,the real answer is: 3\nthe classifier came back with: 3,the real answer is: 3\nthe classifier came back with: 1,the real answer is: 1\nthe classifier came back with: 3,the real answer is: 2\nthe classifier came back with: 1,the real answer is: 3\nthe classifier came back with: 3,the real answer is: 3\nthe classifier came back with: 3,the real answer is: 3\nthe classifier came back with: 3,the real answer is: 1\nthe classifier came back with: 3,the real answer is: 1\nthe classifier came back with: 3,the real answer is: 1\nthe classifier came back with: 1,the real answer is: 1\nthe classifier came back with: 1,the real answer is: 2\nthe classifier came back with: 1,the real answer is: 2\nthe classifier came back with: 2,the real answer is: 1\nthe classifier came back with: 3,the real answer is: 3\nthe classifier came back with: 1,the real answer is: 2\nthe classifier came back with: 3,the real answer is: 2\nthe classifier came back with: 1,the real answer is: 2\nthe classifier came back with: 1,the real answer is: 2\nthe classifier came back with: 1,the real answer is: 3\nthe classifier came back with: 1,the real answer is: 1\nthe classifier came back with: 1,the real answer is: 2\nthe classifier came back with: 1,the real answer is: 1\nthe classifier came back with: 1,the real answer is: 2\nthe classifier came back with: 1,the real answer is: 2\nthe classifier came back with: 1,the real answer is: 2\nthe classifier came back with: 1,the real answer is: 2\nthe classifier came back with: 1,the real answer is: 2\nthe classifier came back with: 2,the real answer is: 3\nthe classifier came back with: 1,the real answer is: 2\nthe classifier came back with: 3,the real answer is: 3\nthe classifier came back with: 1,the real answer is: 1\nthe classifier came back with: 1,the real answer is: 2\nthe classifier came back with: 3,the real answer is: 3\nthe classifier came back with: 3,the real answer is: 2\nthe classifier came back with: 1,the real answer is: 2\nthe classifier came back with: 1,the real answer is: 1\nthe classifier came back with: 1,the real answer is: 3\nthe classifier came back with: 2,the real answer is: 1\nthe classifier came back with: 1,the real answer is: 1\nthe classifier came back with: 1,the real answer is: 3\nthe classifier came back with: 1,the real answer is: 3\nthe classifier came back with: 1,the real answer is: 1\nthe classifier came back with: 1,the real answer is: 2\nthe classifier came back with: 3,the real answer is: 3\nthe classifier came back with: 1,the real answer is: 1\nthe classifier came back with: 3,the real answer is: 3\nthe classifier came back with: 1,the real answer is: 1\nthe classifier came back with: 1,the real answer is: 2\nthe classifier came back with: 1,the real answer is: 2\nthe classifier came back with: 2,the real answer is: 1\nthe classifier came back with: 1,the real answer is: 1\nthe classifier came back with: 1,the real answer is: 3\nthe classifier came back with: 1,the real answer is: 3\nthe classifier came back with: 2,the real answer is: 1\nthe classifier came back with: 1,the real answer is: 2\nthe classifier came back with: 1,the real answer is: 1\nthe classifier came back with: 1,the real answer is: 3\nthe classifier came back with: 1,the real answer is: 3\nthe classifier came back with: 1,the real answer is: 2\nthe classifier came back with: 3,the real answer is: 1\nthe classifier came back with: 1,the real answer is: 1\nthe total error rate is: 0.600000</code></pre><h1 id=\"Done-And-thank-you-for-watching\"><a href=\"#Done-And-thank-you-for-watching\" class=\"headerlink\" title=\"Done,And thank you for watching!\"></a>Done,And thank you for watching!</h1>","text":"K临近算法概述简单地说,k临近算法就是采用不同的特征值之间的距离方法进行分类. 通过数据与数据集间的距离进行分类,以及断定新数据的类别. 这里我们选择使用欧氏距离来当做两点间的距离.实现KNN算法伪码对未知类别属性的数据集中的每个点依次执行以下操作计算已知类别数据集中的点 按照距","link":"","raw":null,"photos":[],"categories":[{"name":"K-邻近","slug":"K-邻近","count":1,"path":"api/categories/K-邻近.json"},{"name":"Python","slug":"K-邻近/Python","count":1,"path":"api/categories/K-邻近/Python.json"},{"name":"机器学习","slug":"K-邻近/Python/机器学习","count":1,"path":"api/categories/K-邻近/Python/机器学习.json"}],"tags":[{"name":"Python","slug":"Python","count":65,"path":"api/tags/Python.json"},{"name":"机器学习","slug":"机器学习","count":4,"path":"api/tags/机器学习.json"},{"name":"K-邻近","slug":"K-邻近","count":1,"path":"api/tags/K-邻近.json"}]}]}